{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b86178bb-5045-48e2-bc6c-5a41b7505a22",
   "metadata": {},
   "source": [
    "# Baseline of 2-layer MLP on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79514043-12da-4230-a8e5-1ab8114d8f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from tqdm import notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf46a2-9d6e-4b83-9b41-169e61f1a126",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2737cb58-0d5c-4250-8a2b-20b1441e71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST('./data', transform=torchvision.transforms.ToTensor(), download=True, train=True)\n",
    "test_dataset  = torchvision.datasets.MNIST('./data', transform=torchvision.transforms.ToTensor(), download=True, train=False)\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "test_loader   = torch.utils.data.DataLoader(test_dataset , batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a748961a-e086-423a-bd63-730c730f7fdf",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49d8bba3-0a21-44b4-9769-bc8eade86faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size = 28*28, layer1_size = 64, num_class = 10, dropout = 0.5):\n",
    "        super(Vanilla_MLP, self).__init__()\n",
    "        self.layer0  = torch.nn.Linear(input_size, layer1_size)\n",
    "        self.layer1  = torch.nn.Linear(layer1_size, num_class)\n",
    "        self.relu    = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p = dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.layer0(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39257c1b-f505-4e19-883e-88577904ca39",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df9b1dcf-f35c-4b6d-bc7a-62c391871a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92098d8c79aa4f499b48bef6edeee5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01  batch:    0  loss: 2.3009\n",
      "epoch: 01  batch:  100  loss: 2.0039\n",
      "epoch: 01  batch:  200  loss: 1.6744\n",
      "epoch: 01  batch:  300  loss: 1.4420\n",
      "epoch: 01  batch:  400  loss: 1.1427\n",
      "epoch: 01  batch:  500  loss: 1.0884\n",
      "epoch: 01  batch:  600  loss: 1.0343\n",
      "epoch: 01  batch:  700  loss: 1.1878\n",
      "epoch: 01  batch:  800  loss: 0.8708\n",
      "epoch: 01  batch:  900  loss: 0.7060\n",
      "epoch: 01  avg train loss: 1.2769  avg train accu: 0.669\n",
      "epoch: 01  avg test  loss: 0.6367  avg test  accu: 0.868\n",
      "==================================================================================\n",
      "epoch: 02  batch:    0  loss: 0.9209\n",
      "epoch: 02  batch:  100  loss: 0.7832\n",
      "epoch: 02  batch:  200  loss: 0.6648\n",
      "epoch: 02  batch:  300  loss: 0.6434\n",
      "epoch: 02  batch:  400  loss: 0.8111\n",
      "epoch: 02  batch:  500  loss: 0.5663\n",
      "epoch: 02  batch:  600  loss: 0.8244\n",
      "epoch: 02  batch:  700  loss: 0.5685\n",
      "epoch: 02  batch:  800  loss: 0.7471\n",
      "epoch: 02  batch:  900  loss: 0.5069\n",
      "epoch: 02  avg train loss: 0.6813  avg train accu: 0.809\n",
      "epoch: 02  avg test  loss: 0.4444  avg test  accu: 0.895\n",
      "==================================================================================\n",
      "epoch: 03  batch:    0  loss: 0.4499\n",
      "epoch: 03  batch:  100  loss: 0.7544\n",
      "epoch: 03  batch:  200  loss: 0.6103\n",
      "epoch: 03  batch:  300  loss: 0.5884\n",
      "epoch: 03  batch:  400  loss: 0.5180\n",
      "epoch: 03  batch:  500  loss: 0.4404\n",
      "epoch: 03  batch:  600  loss: 0.8026\n",
      "epoch: 03  batch:  700  loss: 0.4017\n",
      "epoch: 03  batch:  800  loss: 0.4437\n",
      "epoch: 03  batch:  900  loss: 0.4627\n",
      "epoch: 03  avg train loss: 0.5624  avg train accu: 0.841\n",
      "epoch: 03  avg test  loss: 0.3770  avg test  accu: 0.904\n",
      "==================================================================================\n",
      "epoch: 04  batch:    0  loss: 0.5519\n",
      "epoch: 04  batch:  100  loss: 0.5803\n",
      "epoch: 04  batch:  200  loss: 0.4475\n",
      "epoch: 04  batch:  300  loss: 0.4505\n",
      "epoch: 04  batch:  400  loss: 0.5693\n",
      "epoch: 04  batch:  500  loss: 0.6886\n",
      "epoch: 04  batch:  600  loss: 0.4669\n",
      "epoch: 04  batch:  700  loss: 0.4397\n",
      "epoch: 04  batch:  800  loss: 0.5824\n",
      "epoch: 04  batch:  900  loss: 0.4036\n",
      "epoch: 04  avg train loss: 0.5029  avg train accu: 0.858\n",
      "epoch: 04  avg test  loss: 0.3398  avg test  accu: 0.911\n",
      "==================================================================================\n",
      "epoch: 05  batch:    0  loss: 0.3651\n",
      "epoch: 05  batch:  100  loss: 0.4764\n",
      "epoch: 05  batch:  200  loss: 0.4487\n",
      "epoch: 05  batch:  300  loss: 0.6983\n",
      "epoch: 05  batch:  400  loss: 0.2457\n",
      "epoch: 05  batch:  500  loss: 0.4845\n",
      "epoch: 05  batch:  600  loss: 0.5519\n",
      "epoch: 05  batch:  700  loss: 0.4780\n",
      "epoch: 05  batch:  800  loss: 0.3919\n",
      "epoch: 05  batch:  900  loss: 0.6163\n",
      "epoch: 05  avg train loss: 0.4685  avg train accu: 0.867\n",
      "epoch: 05  avg test  loss: 0.3161  avg test  accu: 0.915\n",
      "==================================================================================\n",
      "epoch: 06  batch:    0  loss: 0.4109\n",
      "epoch: 06  batch:  100  loss: 0.4329\n",
      "epoch: 06  batch:  200  loss: 0.4006\n",
      "epoch: 06  batch:  300  loss: 0.4633\n",
      "epoch: 06  batch:  400  loss: 0.4383\n",
      "epoch: 06  batch:  500  loss: 0.5192\n",
      "epoch: 06  batch:  600  loss: 0.7682\n",
      "epoch: 06  batch:  700  loss: 0.5274\n",
      "epoch: 06  batch:  800  loss: 0.5560\n",
      "epoch: 06  batch:  900  loss: 0.2889\n",
      "epoch: 06  avg train loss: 0.4413  avg train accu: 0.875\n",
      "epoch: 06  avg test  loss: 0.2978  avg test  accu: 0.918\n",
      "==================================================================================\n",
      "epoch: 07  batch:    0  loss: 0.3681\n",
      "epoch: 07  batch:  100  loss: 0.3128\n",
      "epoch: 07  batch:  200  loss: 0.3492\n",
      "epoch: 07  batch:  300  loss: 0.3411\n",
      "epoch: 07  batch:  400  loss: 0.3061\n",
      "epoch: 07  batch:  500  loss: 0.4217\n",
      "epoch: 07  batch:  600  loss: 0.4006\n",
      "epoch: 07  batch:  700  loss: 0.4011\n",
      "epoch: 07  batch:  800  loss: 0.4578\n",
      "epoch: 07  batch:  900  loss: 0.4251\n",
      "epoch: 07  avg train loss: 0.4239  avg train accu: 0.881\n",
      "epoch: 07  avg test  loss: 0.2850  avg test  accu: 0.922\n",
      "==================================================================================\n",
      "epoch: 08  batch:    0  loss: 0.5897\n",
      "epoch: 08  batch:  100  loss: 0.4528\n",
      "epoch: 08  batch:  200  loss: 0.6277\n",
      "epoch: 08  batch:  300  loss: 0.3882\n",
      "epoch: 08  batch:  400  loss: 0.3586\n",
      "epoch: 08  batch:  500  loss: 0.4215\n",
      "epoch: 08  batch:  600  loss: 0.4388\n",
      "epoch: 08  batch:  700  loss: 0.4434\n",
      "epoch: 08  batch:  800  loss: 0.2550\n",
      "epoch: 08  batch:  900  loss: 0.3123\n",
      "epoch: 08  avg train loss: 0.4070  avg train accu: 0.885\n",
      "epoch: 08  avg test  loss: 0.2723  avg test  accu: 0.924\n",
      "==================================================================================\n",
      "epoch: 09  batch:    0  loss: 0.2638\n",
      "epoch: 09  batch:  100  loss: 0.4354\n",
      "epoch: 09  batch:  200  loss: 0.2814\n",
      "epoch: 09  batch:  300  loss: 0.3195\n",
      "epoch: 09  batch:  400  loss: 0.3595\n",
      "epoch: 09  batch:  500  loss: 0.4198\n",
      "epoch: 09  batch:  600  loss: 0.4241\n",
      "epoch: 09  batch:  700  loss: 0.3606\n",
      "epoch: 09  batch:  800  loss: 0.3966\n",
      "epoch: 09  batch:  900  loss: 0.4676\n",
      "epoch: 09  avg train loss: 0.3956  avg train accu: 0.888\n",
      "epoch: 09  avg test  loss: 0.2631  avg test  accu: 0.926\n",
      "==================================================================================\n",
      "epoch: 10  batch:    0  loss: 0.3611\n",
      "epoch: 10  batch:  100  loss: 0.3432\n",
      "epoch: 10  batch:  200  loss: 0.5094\n",
      "epoch: 10  batch:  300  loss: 0.3571\n",
      "epoch: 10  batch:  400  loss: 0.2634\n",
      "epoch: 10  batch:  500  loss: 0.3866\n",
      "epoch: 10  batch:  600  loss: 0.3065\n",
      "epoch: 10  batch:  700  loss: 0.5021\n",
      "epoch: 10  batch:  800  loss: 0.2733\n",
      "epoch: 10  batch:  900  loss: 0.4166\n",
      "epoch: 10  avg train loss: 0.3843  avg train accu: 0.892\n",
      "epoch: 10  avg test  loss: 0.2546  avg test  accu: 0.929\n",
      "==================================================================================\n",
      "epoch: 11  batch:    0  loss: 0.3875\n",
      "epoch: 11  batch:  100  loss: 0.2532\n",
      "epoch: 11  batch:  200  loss: 0.3543\n",
      "epoch: 11  batch:  300  loss: 0.3516\n",
      "epoch: 11  batch:  400  loss: 0.3296\n",
      "epoch: 11  batch:  500  loss: 0.4184\n",
      "epoch: 11  batch:  600  loss: 0.3177\n",
      "epoch: 11  batch:  700  loss: 0.3519\n",
      "epoch: 11  batch:  800  loss: 0.2465\n",
      "epoch: 11  batch:  900  loss: 0.3734\n",
      "epoch: 11  avg train loss: 0.3747  avg train accu: 0.894\n",
      "epoch: 11  avg test  loss: 0.2474  avg test  accu: 0.931\n",
      "==================================================================================\n",
      "epoch: 12  batch:    0  loss: 0.3433\n",
      "epoch: 12  batch:  100  loss: 0.2416\n",
      "epoch: 12  batch:  200  loss: 0.4564\n",
      "epoch: 12  batch:  300  loss: 0.3461\n",
      "epoch: 12  batch:  400  loss: 0.3080\n",
      "epoch: 12  batch:  500  loss: 0.4773\n",
      "epoch: 12  batch:  600  loss: 0.4335\n",
      "epoch: 12  batch:  700  loss: 0.5487\n",
      "epoch: 12  batch:  800  loss: 0.2869\n",
      "epoch: 12  batch:  900  loss: 0.4664\n",
      "epoch: 12  avg train loss: 0.3643  avg train accu: 0.898\n",
      "epoch: 12  avg test  loss: 0.2405  avg test  accu: 0.934\n",
      "==================================================================================\n",
      "epoch: 13  batch:    0  loss: 0.4781\n",
      "epoch: 13  batch:  100  loss: 0.4430\n",
      "epoch: 13  batch:  200  loss: 0.4907\n",
      "epoch: 13  batch:  300  loss: 0.2859\n",
      "epoch: 13  batch:  400  loss: 0.4018\n",
      "epoch: 13  batch:  500  loss: 0.1937\n",
      "epoch: 13  batch:  600  loss: 0.2857\n",
      "epoch: 13  batch:  700  loss: 0.2428\n",
      "epoch: 13  batch:  800  loss: 0.2157\n",
      "epoch: 13  batch:  900  loss: 0.3938\n",
      "epoch: 13  avg train loss: 0.3577  avg train accu: 0.899\n",
      "epoch: 13  avg test  loss: 0.2335  avg test  accu: 0.935\n",
      "==================================================================================\n",
      "epoch: 14  batch:    0  loss: 0.3998\n",
      "epoch: 14  batch:  100  loss: 0.1887\n",
      "epoch: 14  batch:  200  loss: 0.2771\n",
      "epoch: 14  batch:  300  loss: 0.2989\n",
      "epoch: 14  batch:  400  loss: 0.3075\n",
      "epoch: 14  batch:  500  loss: 0.3312\n",
      "epoch: 14  batch:  600  loss: 0.2387\n",
      "epoch: 14  batch:  700  loss: 0.2765\n",
      "epoch: 14  batch:  800  loss: 0.4088\n",
      "epoch: 14  batch:  900  loss: 0.3049\n",
      "epoch: 14  avg train loss: 0.3506  avg train accu: 0.902\n",
      "epoch: 14  avg test  loss: 0.2282  avg test  accu: 0.938\n",
      "==================================================================================\n",
      "epoch: 15  batch:    0  loss: 0.2253\n",
      "epoch: 15  batch:  100  loss: 0.3645\n",
      "epoch: 15  batch:  200  loss: 0.2984\n",
      "epoch: 15  batch:  300  loss: 0.5277\n",
      "epoch: 15  batch:  400  loss: 0.2517\n",
      "epoch: 15  batch:  500  loss: 0.2982\n",
      "epoch: 15  batch:  600  loss: 0.2213\n",
      "epoch: 15  batch:  700  loss: 0.1905\n",
      "epoch: 15  batch:  800  loss: 0.1999\n",
      "epoch: 15  batch:  900  loss: 0.2906\n",
      "epoch: 15  avg train loss: 0.3431  avg train accu: 0.903\n",
      "epoch: 15  avg test  loss: 0.2234  avg test  accu: 0.938\n",
      "==================================================================================\n",
      "epoch: 16  batch:    0  loss: 0.2323\n",
      "epoch: 16  batch:  100  loss: 0.6425\n",
      "epoch: 16  batch:  200  loss: 0.3490\n",
      "epoch: 16  batch:  300  loss: 0.4143\n",
      "epoch: 16  batch:  400  loss: 0.2572\n",
      "epoch: 16  batch:  500  loss: 0.3220\n",
      "epoch: 16  batch:  600  loss: 0.4216\n",
      "epoch: 16  batch:  700  loss: 0.2167\n",
      "epoch: 16  batch:  800  loss: 0.3752\n",
      "epoch: 16  batch:  900  loss: 0.1509\n",
      "epoch: 16  avg train loss: 0.3389  avg train accu: 0.904\n",
      "epoch: 16  avg test  loss: 0.2187  avg test  accu: 0.939\n",
      "==================================================================================\n",
      "epoch: 17  batch:    0  loss: 0.3694\n",
      "epoch: 17  batch:  100  loss: 0.5120\n",
      "epoch: 17  batch:  200  loss: 0.2650\n",
      "epoch: 17  batch:  300  loss: 0.2113\n",
      "epoch: 17  batch:  400  loss: 0.1929\n",
      "epoch: 17  batch:  500  loss: 0.2443\n",
      "epoch: 17  batch:  600  loss: 0.2192\n",
      "epoch: 17  batch:  700  loss: 0.3187\n",
      "epoch: 17  batch:  800  loss: 0.4771\n",
      "epoch: 17  batch:  900  loss: 0.2048\n",
      "epoch: 17  avg train loss: 0.3332  avg train accu: 0.907\n",
      "epoch: 17  avg test  loss: 0.2143  avg test  accu: 0.940\n",
      "==================================================================================\n",
      "epoch: 18  batch:    0  loss: 0.4542\n",
      "epoch: 18  batch:  100  loss: 0.3358\n",
      "epoch: 18  batch:  200  loss: 0.4053\n",
      "epoch: 18  batch:  300  loss: 0.5555\n",
      "epoch: 18  batch:  400  loss: 0.3472\n",
      "epoch: 18  batch:  500  loss: 0.2703\n",
      "epoch: 18  batch:  600  loss: 0.6214\n",
      "epoch: 18  batch:  700  loss: 0.5803\n",
      "epoch: 18  batch:  800  loss: 0.4117\n",
      "epoch: 18  batch:  900  loss: 0.2571\n",
      "epoch: 18  avg train loss: 0.3295  avg train accu: 0.907\n",
      "epoch: 18  avg test  loss: 0.2113  avg test  accu: 0.941\n",
      "==================================================================================\n",
      "epoch: 19  batch:    0  loss: 0.2886\n",
      "epoch: 19  batch:  100  loss: 0.4132\n",
      "epoch: 19  batch:  200  loss: 0.5395\n",
      "epoch: 19  batch:  300  loss: 0.2777\n",
      "epoch: 19  batch:  400  loss: 0.4231\n",
      "epoch: 19  batch:  500  loss: 0.5805\n",
      "epoch: 19  batch:  600  loss: 0.3292\n",
      "epoch: 19  batch:  700  loss: 0.4016\n",
      "epoch: 19  batch:  800  loss: 0.2976\n",
      "epoch: 19  batch:  900  loss: 0.2563\n",
      "epoch: 19  avg train loss: 0.3263  avg train accu: 0.909\n",
      "epoch: 19  avg test  loss: 0.2078  avg test  accu: 0.941\n",
      "==================================================================================\n",
      "epoch: 20  batch:    0  loss: 0.2341\n",
      "epoch: 20  batch:  100  loss: 0.1848\n",
      "epoch: 20  batch:  200  loss: 0.4704\n",
      "epoch: 20  batch:  300  loss: 0.3803\n",
      "epoch: 20  batch:  400  loss: 0.4854\n",
      "epoch: 20  batch:  500  loss: 0.2358\n",
      "epoch: 20  batch:  600  loss: 0.2863\n",
      "epoch: 20  batch:  700  loss: 0.4300\n",
      "epoch: 20  batch:  800  loss: 0.1700\n",
      "epoch: 20  batch:  900  loss: 0.2516\n",
      "epoch: 20  avg train loss: 0.3215  avg train accu: 0.910\n",
      "epoch: 20  avg test  loss: 0.2035  avg test  accu: 0.943\n",
      "==================================================================================\n",
      "epoch: 21  batch:    0  loss: 0.3453\n",
      "epoch: 21  batch:  100  loss: 0.3233\n",
      "epoch: 21  batch:  200  loss: 0.2924\n",
      "epoch: 21  batch:  300  loss: 0.2874\n",
      "epoch: 21  batch:  400  loss: 0.3096\n",
      "epoch: 21  batch:  500  loss: 0.4251\n",
      "epoch: 21  batch:  600  loss: 0.2588\n",
      "epoch: 21  batch:  700  loss: 0.3905\n",
      "epoch: 21  batch:  800  loss: 0.3619\n",
      "epoch: 21  batch:  900  loss: 0.2808\n",
      "epoch: 21  avg train loss: 0.3180  avg train accu: 0.911\n",
      "epoch: 21  avg test  loss: 0.2000  avg test  accu: 0.944\n",
      "==================================================================================\n",
      "epoch: 22  batch:    0  loss: 0.4542\n",
      "epoch: 22  batch:  100  loss: 0.2277\n",
      "epoch: 22  batch:  200  loss: 0.5351\n",
      "epoch: 22  batch:  300  loss: 0.3995\n",
      "epoch: 22  batch:  400  loss: 0.3460\n",
      "epoch: 22  batch:  500  loss: 0.2235\n",
      "epoch: 22  batch:  600  loss: 0.2825\n",
      "epoch: 22  batch:  700  loss: 0.3973\n",
      "epoch: 22  batch:  800  loss: 0.2742\n",
      "epoch: 22  batch:  900  loss: 0.2331\n",
      "epoch: 22  avg train loss: 0.3150  avg train accu: 0.910\n",
      "epoch: 22  avg test  loss: 0.1986  avg test  accu: 0.944\n",
      "==================================================================================\n",
      "epoch: 23  batch:    0  loss: 0.2452\n",
      "epoch: 23  batch:  100  loss: 0.2852\n",
      "epoch: 23  batch:  200  loss: 0.2625\n",
      "epoch: 23  batch:  300  loss: 0.3737\n",
      "epoch: 23  batch:  400  loss: 0.2957\n",
      "epoch: 23  batch:  500  loss: 0.2918\n",
      "epoch: 23  batch:  600  loss: 0.2519\n",
      "epoch: 23  batch:  700  loss: 0.2713\n",
      "epoch: 23  batch:  800  loss: 0.3605\n",
      "epoch: 23  batch:  900  loss: 0.2631\n",
      "epoch: 23  avg train loss: 0.3125  avg train accu: 0.913\n",
      "epoch: 23  avg test  loss: 0.1958  avg test  accu: 0.945\n",
      "==================================================================================\n",
      "epoch: 24  batch:    0  loss: 0.3973\n",
      "epoch: 24  batch:  100  loss: 0.3398\n",
      "epoch: 24  batch:  200  loss: 0.3335\n",
      "epoch: 24  batch:  300  loss: 0.2366\n",
      "epoch: 24  batch:  400  loss: 0.1657\n",
      "epoch: 24  batch:  500  loss: 0.3602\n",
      "epoch: 24  batch:  600  loss: 0.2837\n",
      "epoch: 24  batch:  700  loss: 0.2567\n",
      "epoch: 24  batch:  800  loss: 0.3218\n",
      "epoch: 24  batch:  900  loss: 0.3116\n",
      "epoch: 24  avg train loss: 0.3084  avg train accu: 0.913\n",
      "epoch: 24  avg test  loss: 0.1931  avg test  accu: 0.946\n",
      "==================================================================================\n",
      "epoch: 25  batch:    0  loss: 0.2180\n",
      "epoch: 25  batch:  100  loss: 0.1511\n",
      "epoch: 25  batch:  200  loss: 0.6521\n",
      "epoch: 25  batch:  300  loss: 0.3392\n",
      "epoch: 25  batch:  400  loss: 0.2202\n",
      "epoch: 25  batch:  500  loss: 0.2866\n",
      "epoch: 25  batch:  600  loss: 0.1949\n",
      "epoch: 25  batch:  700  loss: 0.2687\n",
      "epoch: 25  batch:  800  loss: 0.3456\n",
      "epoch: 25  batch:  900  loss: 0.1420\n",
      "epoch: 25  avg train loss: 0.3047  avg train accu: 0.913\n",
      "epoch: 25  avg test  loss: 0.1900  avg test  accu: 0.947\n",
      "==================================================================================\n",
      "epoch: 26  batch:    0  loss: 0.2337\n",
      "epoch: 26  batch:  100  loss: 0.3209\n",
      "epoch: 26  batch:  200  loss: 0.2703\n",
      "epoch: 26  batch:  300  loss: 0.2858\n",
      "epoch: 26  batch:  400  loss: 0.1932\n",
      "epoch: 26  batch:  500  loss: 0.3198\n",
      "epoch: 26  batch:  600  loss: 0.3354\n",
      "epoch: 26  batch:  700  loss: 0.4884\n",
      "epoch: 26  batch:  800  loss: 0.2744\n",
      "epoch: 26  batch:  900  loss: 0.2523\n",
      "epoch: 26  avg train loss: 0.3024  avg train accu: 0.915\n",
      "epoch: 26  avg test  loss: 0.1888  avg test  accu: 0.947\n",
      "==================================================================================\n",
      "epoch: 27  batch:    0  loss: 0.5045\n",
      "epoch: 27  batch:  100  loss: 0.5062\n",
      "epoch: 27  batch:  200  loss: 0.3099\n",
      "epoch: 27  batch:  300  loss: 0.3135\n",
      "epoch: 27  batch:  400  loss: 0.2115\n",
      "epoch: 27  batch:  500  loss: 0.2035\n",
      "epoch: 27  batch:  600  loss: 0.2250\n",
      "epoch: 27  batch:  700  loss: 0.2921\n",
      "epoch: 27  batch:  800  loss: 0.3291\n",
      "epoch: 27  batch:  900  loss: 0.4004\n",
      "epoch: 27  avg train loss: 0.3007  avg train accu: 0.914\n",
      "epoch: 27  avg test  loss: 0.1860  avg test  accu: 0.947\n",
      "==================================================================================\n",
      "epoch: 28  batch:    0  loss: 0.2087\n",
      "epoch: 28  batch:  100  loss: 0.2262\n",
      "epoch: 28  batch:  200  loss: 0.2891\n",
      "epoch: 28  batch:  300  loss: 0.3621\n",
      "epoch: 28  batch:  400  loss: 0.4736\n",
      "epoch: 28  batch:  500  loss: 0.4416\n",
      "epoch: 28  batch:  600  loss: 0.2598\n",
      "epoch: 28  batch:  700  loss: 0.2100\n",
      "epoch: 28  batch:  800  loss: 0.3035\n",
      "epoch: 28  batch:  900  loss: 0.4569\n",
      "epoch: 28  avg train loss: 0.2991  avg train accu: 0.917\n",
      "epoch: 28  avg test  loss: 0.1840  avg test  accu: 0.948\n",
      "==================================================================================\n",
      "epoch: 29  batch:    0  loss: 0.1819\n",
      "epoch: 29  batch:  100  loss: 0.2297\n",
      "epoch: 29  batch:  200  loss: 0.3141\n",
      "epoch: 29  batch:  300  loss: 0.2572\n",
      "epoch: 29  batch:  400  loss: 0.4136\n",
      "epoch: 29  batch:  500  loss: 0.2644\n",
      "epoch: 29  batch:  600  loss: 0.4761\n",
      "epoch: 29  batch:  700  loss: 0.2535\n",
      "epoch: 29  batch:  800  loss: 0.2099\n",
      "epoch: 29  batch:  900  loss: 0.4433\n",
      "epoch: 29  avg train loss: 0.2960  avg train accu: 0.916\n",
      "epoch: 29  avg test  loss: 0.1822  avg test  accu: 0.949\n",
      "==================================================================================\n",
      "epoch: 30  batch:    0  loss: 0.3614\n",
      "epoch: 30  batch:  100  loss: 0.4578\n",
      "epoch: 30  batch:  200  loss: 0.1897\n",
      "epoch: 30  batch:  300  loss: 0.3890\n",
      "epoch: 30  batch:  400  loss: 0.2679\n",
      "epoch: 30  batch:  500  loss: 0.2800\n",
      "epoch: 30  batch:  600  loss: 0.3373\n",
      "epoch: 30  batch:  700  loss: 0.3215\n",
      "epoch: 30  batch:  800  loss: 0.4280\n",
      "epoch: 30  batch:  900  loss: 0.2008\n",
      "epoch: 30  avg train loss: 0.2934  avg train accu: 0.918\n",
      "epoch: 30  avg test  loss: 0.1798  avg test  accu: 0.949\n",
      "==================================================================================\n",
      "epoch: 31  batch:    0  loss: 0.2267\n",
      "epoch: 31  batch:  100  loss: 0.2614\n",
      "epoch: 31  batch:  200  loss: 0.2743\n",
      "epoch: 31  batch:  300  loss: 0.2380\n",
      "epoch: 31  batch:  400  loss: 0.2023\n",
      "epoch: 31  batch:  500  loss: 0.1812\n",
      "epoch: 31  batch:  600  loss: 0.2371\n",
      "epoch: 31  batch:  700  loss: 0.2702\n",
      "epoch: 31  batch:  800  loss: 0.1867\n",
      "epoch: 31  batch:  900  loss: 0.1523\n",
      "epoch: 31  avg train loss: 0.2881  avg train accu: 0.918\n",
      "epoch: 31  avg test  loss: 0.1784  avg test  accu: 0.950\n",
      "==================================================================================\n",
      "epoch: 32  batch:    0  loss: 0.2660\n",
      "epoch: 32  batch:  100  loss: 0.2045\n",
      "epoch: 32  batch:  200  loss: 0.3097\n",
      "epoch: 32  batch:  300  loss: 0.2357\n",
      "epoch: 32  batch:  400  loss: 0.3104\n",
      "epoch: 32  batch:  500  loss: 0.1957\n",
      "epoch: 32  batch:  600  loss: 0.3519\n",
      "epoch: 32  batch:  700  loss: 0.2573\n",
      "epoch: 32  batch:  800  loss: 0.1689\n",
      "epoch: 32  batch:  900  loss: 0.3583\n",
      "epoch: 32  avg train loss: 0.2909  avg train accu: 0.918\n",
      "epoch: 32  avg test  loss: 0.1772  avg test  accu: 0.950\n",
      "==================================================================================\n",
      "epoch: 33  batch:    0  loss: 0.3249\n",
      "epoch: 33  batch:  100  loss: 0.3626\n",
      "epoch: 33  batch:  200  loss: 0.5687\n",
      "epoch: 33  batch:  300  loss: 0.2374\n",
      "epoch: 33  batch:  400  loss: 0.2513\n",
      "epoch: 33  batch:  500  loss: 0.2344\n",
      "epoch: 33  batch:  600  loss: 0.4341\n",
      "epoch: 33  batch:  700  loss: 0.2843\n",
      "epoch: 33  batch:  800  loss: 0.3504\n",
      "epoch: 33  batch:  900  loss: 0.2977\n",
      "epoch: 33  avg train loss: 0.2896  avg train accu: 0.919\n",
      "epoch: 33  avg test  loss: 0.1767  avg test  accu: 0.950\n",
      "==================================================================================\n",
      "epoch: 34  batch:    0  loss: 0.2849\n",
      "epoch: 34  batch:  100  loss: 0.2236\n",
      "epoch: 34  batch:  200  loss: 0.4633\n",
      "epoch: 34  batch:  300  loss: 0.1364\n",
      "epoch: 34  batch:  400  loss: 0.2598\n",
      "epoch: 34  batch:  500  loss: 0.1448\n",
      "epoch: 34  batch:  600  loss: 0.4088\n",
      "epoch: 34  batch:  700  loss: 0.3200\n",
      "epoch: 34  batch:  800  loss: 0.2404\n",
      "epoch: 34  batch:  900  loss: 0.2463\n",
      "epoch: 34  avg train loss: 0.2869  avg train accu: 0.920\n",
      "epoch: 34  avg test  loss: 0.1754  avg test  accu: 0.951\n",
      "==================================================================================\n",
      "epoch: 35  batch:    0  loss: 0.3324\n",
      "epoch: 35  batch:  100  loss: 0.1881\n",
      "epoch: 35  batch:  200  loss: 0.3195\n",
      "epoch: 35  batch:  300  loss: 0.2488\n",
      "epoch: 35  batch:  400  loss: 0.1941\n",
      "epoch: 35  batch:  500  loss: 0.5323\n",
      "epoch: 35  batch:  600  loss: 0.2451\n",
      "epoch: 35  batch:  700  loss: 0.3206\n",
      "epoch: 35  batch:  800  loss: 0.3831\n",
      "epoch: 35  batch:  900  loss: 0.4218\n",
      "epoch: 35  avg train loss: 0.2852  avg train accu: 0.920\n",
      "epoch: 35  avg test  loss: 0.1731  avg test  accu: 0.951\n",
      "==================================================================================\n",
      "epoch: 36  batch:    0  loss: 0.3897\n",
      "epoch: 36  batch:  100  loss: 0.2021\n",
      "epoch: 36  batch:  200  loss: 0.1268\n",
      "epoch: 36  batch:  300  loss: 0.1590\n",
      "epoch: 36  batch:  400  loss: 0.2861\n",
      "epoch: 36  batch:  500  loss: 0.2520\n",
      "epoch: 36  batch:  600  loss: 0.3052\n",
      "epoch: 36  batch:  700  loss: 0.2504\n",
      "epoch: 36  batch:  800  loss: 0.3230\n",
      "epoch: 36  batch:  900  loss: 0.3622\n",
      "epoch: 36  avg train loss: 0.2833  avg train accu: 0.920\n",
      "epoch: 36  avg test  loss: 0.1723  avg test  accu: 0.951\n",
      "==================================================================================\n",
      "epoch: 37  batch:    0  loss: 0.2366\n",
      "epoch: 37  batch:  100  loss: 0.2240\n",
      "epoch: 37  batch:  200  loss: 0.3507\n",
      "epoch: 37  batch:  300  loss: 0.1832\n",
      "epoch: 37  batch:  400  loss: 0.1358\n",
      "epoch: 37  batch:  500  loss: 0.3402\n",
      "epoch: 37  batch:  600  loss: 0.1170\n",
      "epoch: 37  batch:  700  loss: 0.3021\n",
      "epoch: 37  batch:  800  loss: 0.2216\n",
      "epoch: 37  batch:  900  loss: 0.3119\n",
      "epoch: 37  avg train loss: 0.2851  avg train accu: 0.920\n",
      "epoch: 37  avg test  loss: 0.1725  avg test  accu: 0.952\n",
      "==================================================================================\n",
      "epoch: 38  batch:    0  loss: 0.1917\n",
      "epoch: 38  batch:  100  loss: 0.4798\n",
      "epoch: 38  batch:  200  loss: 0.2390\n",
      "epoch: 38  batch:  300  loss: 0.1640\n",
      "epoch: 38  batch:  400  loss: 0.2755\n",
      "epoch: 38  batch:  500  loss: 0.1458\n",
      "epoch: 38  batch:  600  loss: 0.2514\n",
      "epoch: 38  batch:  700  loss: 0.1959\n",
      "epoch: 38  batch:  800  loss: 0.2521\n",
      "epoch: 38  batch:  900  loss: 0.2963\n",
      "epoch: 38  avg train loss: 0.2825  avg train accu: 0.921\n",
      "epoch: 38  avg test  loss: 0.1696  avg test  accu: 0.953\n",
      "==================================================================================\n",
      "epoch: 39  batch:    0  loss: 0.3024\n",
      "epoch: 39  batch:  100  loss: 0.2263\n",
      "epoch: 39  batch:  200  loss: 0.3694\n",
      "epoch: 39  batch:  300  loss: 0.1718\n",
      "epoch: 39  batch:  400  loss: 0.2755\n",
      "epoch: 39  batch:  500  loss: 0.2480\n",
      "epoch: 39  batch:  600  loss: 0.3511\n",
      "epoch: 39  batch:  700  loss: 0.1295\n",
      "epoch: 39  batch:  800  loss: 0.2161\n",
      "epoch: 39  batch:  900  loss: 0.2509\n",
      "epoch: 39  avg train loss: 0.2814  avg train accu: 0.921\n",
      "epoch: 39  avg test  loss: 0.1686  avg test  accu: 0.953\n",
      "==================================================================================\n",
      "epoch: 40  batch:    0  loss: 0.2466\n",
      "epoch: 40  batch:  100  loss: 0.2974\n",
      "epoch: 40  batch:  200  loss: 0.2258\n",
      "epoch: 40  batch:  300  loss: 0.2239\n",
      "epoch: 40  batch:  400  loss: 0.3542\n",
      "epoch: 40  batch:  500  loss: 0.2492\n",
      "epoch: 40  batch:  600  loss: 0.2724\n",
      "epoch: 40  batch:  700  loss: 0.2067\n",
      "epoch: 40  batch:  800  loss: 0.2839\n",
      "epoch: 40  batch:  900  loss: 0.2624\n",
      "epoch: 40  avg train loss: 0.2792  avg train accu: 0.921\n",
      "epoch: 40  avg test  loss: 0.1676  avg test  accu: 0.952\n",
      "==================================================================================\n",
      "epoch: 41  batch:    0  loss: 0.4365\n",
      "epoch: 41  batch:  100  loss: 0.2559\n",
      "epoch: 41  batch:  200  loss: 0.1776\n",
      "epoch: 41  batch:  300  loss: 0.2107\n",
      "epoch: 41  batch:  400  loss: 0.2978\n",
      "epoch: 41  batch:  500  loss: 0.2945\n",
      "epoch: 41  batch:  600  loss: 0.2399\n",
      "epoch: 41  batch:  700  loss: 0.2156\n",
      "epoch: 41  batch:  800  loss: 0.2489\n",
      "epoch: 41  batch:  900  loss: 0.3189\n",
      "epoch: 41  avg train loss: 0.2786  avg train accu: 0.920\n",
      "epoch: 41  avg test  loss: 0.1663  avg test  accu: 0.953\n",
      "==================================================================================\n",
      "epoch: 42  batch:    0  loss: 0.2491\n",
      "epoch: 42  batch:  100  loss: 0.1846\n",
      "epoch: 42  batch:  200  loss: 0.1640\n",
      "epoch: 42  batch:  300  loss: 0.1790\n",
      "epoch: 42  batch:  400  loss: 0.3117\n",
      "epoch: 42  batch:  500  loss: 0.2792\n",
      "epoch: 42  batch:  600  loss: 0.3367\n",
      "epoch: 42  batch:  700  loss: 0.1177\n",
      "epoch: 42  batch:  800  loss: 0.1839\n",
      "epoch: 42  batch:  900  loss: 0.2440\n",
      "epoch: 42  avg train loss: 0.2747  avg train accu: 0.922\n",
      "epoch: 42  avg test  loss: 0.1653  avg test  accu: 0.954\n",
      "==================================================================================\n",
      "epoch: 43  batch:    0  loss: 0.1742\n",
      "epoch: 43  batch:  100  loss: 0.1805\n",
      "epoch: 43  batch:  200  loss: 0.4254\n",
      "epoch: 43  batch:  300  loss: 0.4381\n",
      "epoch: 43  batch:  400  loss: 0.2219\n",
      "epoch: 43  batch:  500  loss: 0.2880\n",
      "epoch: 43  batch:  600  loss: 0.2997\n",
      "epoch: 43  batch:  700  loss: 0.1915\n",
      "epoch: 43  batch:  800  loss: 0.1559\n",
      "epoch: 43  batch:  900  loss: 0.2257\n",
      "epoch: 43  avg train loss: 0.2757  avg train accu: 0.922\n",
      "epoch: 43  avg test  loss: 0.1650  avg test  accu: 0.954\n",
      "==================================================================================\n",
      "epoch: 44  batch:    0  loss: 0.2288\n",
      "epoch: 44  batch:  100  loss: 0.1541\n",
      "epoch: 44  batch:  200  loss: 0.1971\n",
      "epoch: 44  batch:  300  loss: 0.2467\n",
      "epoch: 44  batch:  400  loss: 0.2098\n",
      "epoch: 44  batch:  500  loss: 0.2368\n",
      "epoch: 44  batch:  600  loss: 0.1983\n",
      "epoch: 44  batch:  700  loss: 0.5205\n",
      "epoch: 44  batch:  800  loss: 0.2472\n",
      "epoch: 44  batch:  900  loss: 0.2076\n",
      "epoch: 44  avg train loss: 0.2757  avg train accu: 0.922\n",
      "epoch: 44  avg test  loss: 0.1644  avg test  accu: 0.953\n",
      "==================================================================================\n",
      "epoch: 45  batch:    0  loss: 0.2909\n",
      "epoch: 45  batch:  100  loss: 0.3733\n",
      "epoch: 45  batch:  200  loss: 0.1754\n",
      "epoch: 45  batch:  300  loss: 0.1994\n",
      "epoch: 45  batch:  400  loss: 0.2374\n",
      "epoch: 45  batch:  500  loss: 0.1580\n",
      "epoch: 45  batch:  600  loss: 0.3735\n",
      "epoch: 45  batch:  700  loss: 0.1572\n",
      "epoch: 45  batch:  800  loss: 0.3861\n",
      "epoch: 45  batch:  900  loss: 0.2031\n",
      "epoch: 45  avg train loss: 0.2733  avg train accu: 0.925\n",
      "epoch: 45  avg test  loss: 0.1636  avg test  accu: 0.954\n",
      "==================================================================================\n",
      "epoch: 46  batch:    0  loss: 0.2048\n",
      "epoch: 46  batch:  100  loss: 0.1808\n",
      "epoch: 46  batch:  200  loss: 0.3080\n",
      "epoch: 46  batch:  300  loss: 0.2984\n",
      "epoch: 46  batch:  400  loss: 0.2541\n",
      "epoch: 46  batch:  500  loss: 0.3420\n",
      "epoch: 46  batch:  600  loss: 0.1039\n",
      "epoch: 46  batch:  700  loss: 0.1981\n",
      "epoch: 46  batch:  800  loss: 0.2051\n",
      "epoch: 46  batch:  900  loss: 0.4274\n",
      "epoch: 46  avg train loss: 0.2740  avg train accu: 0.922\n",
      "epoch: 46  avg test  loss: 0.1635  avg test  accu: 0.954\n",
      "==================================================================================\n",
      "epoch: 47  batch:    0  loss: 0.2598\n",
      "epoch: 47  batch:  100  loss: 0.2049\n",
      "epoch: 47  batch:  200  loss: 0.2259\n",
      "epoch: 47  batch:  300  loss: 0.2389\n",
      "epoch: 47  batch:  400  loss: 0.4258\n",
      "epoch: 47  batch:  500  loss: 0.1908\n",
      "epoch: 47  batch:  600  loss: 0.2730\n",
      "epoch: 47  batch:  700  loss: 0.2198\n",
      "epoch: 47  batch:  800  loss: 0.5690\n",
      "epoch: 47  batch:  900  loss: 0.2444\n",
      "epoch: 47  avg train loss: 0.2712  avg train accu: 0.924\n",
      "epoch: 47  avg test  loss: 0.1622  avg test  accu: 0.955\n",
      "==================================================================================\n",
      "epoch: 48  batch:    0  loss: 0.1453\n",
      "epoch: 48  batch:  100  loss: 0.2458\n",
      "epoch: 48  batch:  200  loss: 0.2790\n",
      "epoch: 48  batch:  300  loss: 0.1313\n",
      "epoch: 48  batch:  400  loss: 0.4035\n",
      "epoch: 48  batch:  500  loss: 0.2888\n",
      "epoch: 48  batch:  600  loss: 0.2605\n",
      "epoch: 48  batch:  700  loss: 0.1426\n",
      "epoch: 48  batch:  800  loss: 0.2000\n",
      "epoch: 48  batch:  900  loss: 0.1761\n",
      "epoch: 48  avg train loss: 0.2728  avg train accu: 0.923\n",
      "epoch: 48  avg test  loss: 0.1617  avg test  accu: 0.954\n",
      "==================================================================================\n",
      "epoch: 49  batch:    0  loss: 0.1875\n",
      "epoch: 49  batch:  100  loss: 0.2412\n",
      "epoch: 49  batch:  200  loss: 0.2101\n",
      "epoch: 49  batch:  300  loss: 0.2100\n",
      "epoch: 49  batch:  400  loss: 0.1903\n",
      "epoch: 49  batch:  500  loss: 0.5831\n",
      "epoch: 49  batch:  600  loss: 0.4067\n",
      "epoch: 49  batch:  700  loss: 0.4492\n",
      "epoch: 49  batch:  800  loss: 0.1999\n",
      "epoch: 49  batch:  900  loss: 0.2410\n",
      "epoch: 49  avg train loss: 0.2707  avg train accu: 0.923\n",
      "epoch: 49  avg test  loss: 0.1609  avg test  accu: 0.955\n",
      "==================================================================================\n",
      "epoch: 50  batch:    0  loss: 0.2165\n",
      "epoch: 50  batch:  100  loss: 0.3173\n",
      "epoch: 50  batch:  200  loss: 0.2634\n",
      "epoch: 50  batch:  300  loss: 0.2338\n",
      "epoch: 50  batch:  400  loss: 0.2552\n",
      "epoch: 50  batch:  500  loss: 0.2460\n",
      "epoch: 50  batch:  600  loss: 0.3274\n",
      "epoch: 50  batch:  700  loss: 0.2640\n",
      "epoch: 50  batch:  800  loss: 0.2442\n",
      "epoch: 50  batch:  900  loss: 0.2664\n",
      "epoch: 50  avg train loss: 0.2716  avg train accu: 0.926\n",
      "epoch: 50  avg test  loss: 0.1601  avg test  accu: 0.956\n",
      "==================================================================================\n",
      "epoch: 51  batch:    0  loss: 0.1533\n",
      "epoch: 51  batch:  100  loss: 0.3764\n",
      "epoch: 51  batch:  200  loss: 0.2430\n",
      "epoch: 51  batch:  300  loss: 0.2281\n",
      "epoch: 51  batch:  400  loss: 0.2662\n",
      "epoch: 51  batch:  500  loss: 0.2200\n",
      "epoch: 51  batch:  600  loss: 0.1511\n",
      "epoch: 51  batch:  700  loss: 0.1575\n",
      "epoch: 51  batch:  800  loss: 0.3132\n",
      "epoch: 51  batch:  900  loss: 0.2031\n",
      "epoch: 51  avg train loss: 0.2674  avg train accu: 0.924\n",
      "epoch: 51  avg test  loss: 0.1596  avg test  accu: 0.956\n",
      "==================================================================================\n",
      "epoch: 52  batch:    0  loss: 0.2433\n",
      "epoch: 52  batch:  100  loss: 0.2574\n",
      "epoch: 52  batch:  200  loss: 0.1857\n",
      "epoch: 52  batch:  300  loss: 0.2987\n",
      "epoch: 52  batch:  400  loss: 0.3062\n",
      "epoch: 52  batch:  500  loss: 0.2183\n",
      "epoch: 52  batch:  600  loss: 0.2990\n",
      "epoch: 52  batch:  700  loss: 0.2873\n",
      "epoch: 52  batch:  800  loss: 0.1298\n",
      "epoch: 52  batch:  900  loss: 0.3054\n",
      "epoch: 52  avg train loss: 0.2672  avg train accu: 0.925\n",
      "epoch: 52  avg test  loss: 0.1591  avg test  accu: 0.955\n",
      "==================================================================================\n",
      "epoch: 53  batch:    0  loss: 0.1447\n",
      "epoch: 53  batch:  100  loss: 0.2921\n",
      "epoch: 53  batch:  200  loss: 0.2165\n",
      "epoch: 53  batch:  300  loss: 0.2543\n",
      "epoch: 53  batch:  400  loss: 0.3150\n",
      "epoch: 53  batch:  500  loss: 0.2392\n",
      "epoch: 53  batch:  600  loss: 0.2819\n",
      "epoch: 53  batch:  700  loss: 0.2002\n",
      "epoch: 53  batch:  800  loss: 0.2957\n",
      "epoch: 53  batch:  900  loss: 0.1742\n",
      "epoch: 53  avg train loss: 0.2685  avg train accu: 0.924\n",
      "epoch: 53  avg test  loss: 0.1585  avg test  accu: 0.955\n",
      "==================================================================================\n",
      "epoch: 54  batch:    0  loss: 0.2864\n",
      "epoch: 54  batch:  100  loss: 0.3846\n",
      "epoch: 54  batch:  200  loss: 0.2841\n",
      "epoch: 54  batch:  300  loss: 0.2555\n",
      "epoch: 54  batch:  400  loss: 0.3125\n",
      "epoch: 54  batch:  500  loss: 0.2503\n",
      "epoch: 54  batch:  600  loss: 0.3250\n",
      "epoch: 54  batch:  700  loss: 0.3402\n",
      "epoch: 54  batch:  800  loss: 0.3195\n",
      "epoch: 54  batch:  900  loss: 0.2032\n",
      "epoch: 54  avg train loss: 0.2684  avg train accu: 0.925\n",
      "epoch: 54  avg test  loss: 0.1581  avg test  accu: 0.955\n",
      "==================================================================================\n",
      "epoch: 55  batch:    0  loss: 0.2852\n",
      "epoch: 55  batch:  100  loss: 0.2485\n",
      "epoch: 55  batch:  200  loss: 0.3075\n",
      "epoch: 55  batch:  300  loss: 0.1540\n",
      "epoch: 55  batch:  400  loss: 0.2141\n",
      "epoch: 55  batch:  500  loss: 0.2143\n",
      "epoch: 55  batch:  600  loss: 0.3004\n",
      "epoch: 55  batch:  700  loss: 0.1999\n",
      "epoch: 55  batch:  800  loss: 0.2806\n",
      "epoch: 55  batch:  900  loss: 0.1597\n",
      "epoch: 55  avg train loss: 0.2668  avg train accu: 0.924\n",
      "epoch: 55  avg test  loss: 0.1592  avg test  accu: 0.955\n",
      "==================================================================================\n",
      "epoch: 56  batch:    0  loss: 0.3643\n",
      "epoch: 56  batch:  100  loss: 0.2799\n",
      "epoch: 56  batch:  200  loss: 0.1526\n",
      "epoch: 56  batch:  300  loss: 0.3166\n",
      "epoch: 56  batch:  400  loss: 0.2988\n",
      "epoch: 56  batch:  500  loss: 0.4289\n",
      "epoch: 56  batch:  600  loss: 0.2391\n",
      "epoch: 56  batch:  700  loss: 0.2359\n",
      "epoch: 56  batch:  800  loss: 0.1624\n",
      "epoch: 56  batch:  900  loss: 0.2144\n",
      "epoch: 56  avg train loss: 0.2641  avg train accu: 0.925\n",
      "epoch: 56  avg test  loss: 0.1566  avg test  accu: 0.955\n",
      "==================================================================================\n",
      "epoch: 57  batch:    0  loss: 0.2717\n",
      "epoch: 57  batch:  100  loss: 0.0784\n",
      "epoch: 57  batch:  200  loss: 0.2162\n",
      "epoch: 57  batch:  300  loss: 0.1389\n",
      "epoch: 57  batch:  400  loss: 0.3316\n",
      "epoch: 57  batch:  500  loss: 0.3135\n",
      "epoch: 57  batch:  600  loss: 0.2740\n",
      "epoch: 57  batch:  700  loss: 0.2596\n",
      "epoch: 57  batch:  800  loss: 0.2601\n",
      "epoch: 57  batch:  900  loss: 0.3854\n",
      "epoch: 57  avg train loss: 0.2639  avg train accu: 0.925\n",
      "epoch: 57  avg test  loss: 0.1560  avg test  accu: 0.956\n",
      "==================================================================================\n",
      "epoch: 58  batch:    0  loss: 0.2815\n",
      "epoch: 58  batch:  100  loss: 0.2433\n",
      "epoch: 58  batch:  200  loss: 0.6120\n",
      "epoch: 58  batch:  300  loss: 0.2585\n",
      "epoch: 58  batch:  400  loss: 0.1315\n",
      "epoch: 58  batch:  500  loss: 0.2151\n",
      "epoch: 58  batch:  600  loss: 0.2676\n",
      "epoch: 58  batch:  700  loss: 0.1741\n",
      "epoch: 58  batch:  800  loss: 0.2902\n",
      "epoch: 58  batch:  900  loss: 0.3042\n",
      "epoch: 58  avg train loss: 0.2673  avg train accu: 0.924\n",
      "epoch: 58  avg test  loss: 0.1567  avg test  accu: 0.955\n",
      "==================================================================================\n",
      "epoch: 59  batch:    0  loss: 0.3752\n",
      "epoch: 59  batch:  100  loss: 0.3981\n",
      "epoch: 59  batch:  200  loss: 0.2486\n",
      "epoch: 59  batch:  300  loss: 0.1883\n",
      "epoch: 59  batch:  400  loss: 0.2790\n",
      "epoch: 59  batch:  500  loss: 0.3767\n",
      "epoch: 59  batch:  600  loss: 0.2294\n",
      "epoch: 59  batch:  700  loss: 0.1268\n",
      "epoch: 59  batch:  800  loss: 0.1274\n",
      "epoch: 59  batch:  900  loss: 0.2889\n",
      "epoch: 59  avg train loss: 0.2651  avg train accu: 0.925\n",
      "epoch: 59  avg test  loss: 0.1551  avg test  accu: 0.956\n",
      "==================================================================================\n",
      "epoch: 60  batch:    0  loss: 0.2946\n",
      "epoch: 60  batch:  100  loss: 0.2194\n",
      "epoch: 60  batch:  200  loss: 0.3395\n",
      "epoch: 60  batch:  300  loss: 0.2381\n",
      "epoch: 60  batch:  400  loss: 0.1928\n",
      "epoch: 60  batch:  500  loss: 0.3026\n",
      "epoch: 60  batch:  600  loss: 0.2591\n",
      "epoch: 60  batch:  700  loss: 0.2426\n",
      "epoch: 60  batch:  800  loss: 0.3795\n",
      "epoch: 60  batch:  900  loss: 0.3588\n",
      "epoch: 60  avg train loss: 0.2655  avg train accu: 0.925\n",
      "epoch: 60  avg test  loss: 0.1557  avg test  accu: 0.956\n",
      "==================================================================================\n",
      "epoch: 61  batch:    0  loss: 0.3829\n",
      "epoch: 61  batch:  100  loss: 0.3135\n",
      "epoch: 61  batch:  200  loss: 0.1967\n",
      "epoch: 61  batch:  300  loss: 0.1954\n",
      "epoch: 61  batch:  400  loss: 0.1199\n",
      "epoch: 61  batch:  500  loss: 0.2927\n",
      "epoch: 61  batch:  600  loss: 0.3148\n",
      "epoch: 61  batch:  700  loss: 0.4517\n",
      "epoch: 61  batch:  800  loss: 0.2026\n",
      "epoch: 61  batch:  900  loss: 0.1322\n",
      "epoch: 61  avg train loss: 0.2646  avg train accu: 0.926\n",
      "epoch: 61  avg test  loss: 0.1546  avg test  accu: 0.956\n",
      "==================================================================================\n",
      "epoch: 62  batch:    0  loss: 0.1552\n",
      "epoch: 62  batch:  100  loss: 0.2136\n",
      "epoch: 62  batch:  200  loss: 0.3314\n",
      "epoch: 62  batch:  300  loss: 0.2132\n",
      "epoch: 62  batch:  400  loss: 0.2670\n",
      "epoch: 62  batch:  500  loss: 0.2909\n",
      "epoch: 62  batch:  600  loss: 0.1395\n",
      "epoch: 62  batch:  700  loss: 0.1474\n",
      "epoch: 62  batch:  800  loss: 0.1496\n",
      "epoch: 62  batch:  900  loss: 0.1703\n",
      "epoch: 62  avg train loss: 0.2630  avg train accu: 0.926\n",
      "epoch: 62  avg test  loss: 0.1553  avg test  accu: 0.956\n",
      "==================================================================================\n",
      "epoch: 63  batch:    0  loss: 0.2193\n",
      "epoch: 63  batch:  100  loss: 0.2532\n",
      "epoch: 63  batch:  200  loss: 0.2489\n",
      "epoch: 63  batch:  300  loss: 0.3140\n",
      "epoch: 63  batch:  400  loss: 0.2140\n",
      "epoch: 63  batch:  500  loss: 0.4103\n",
      "epoch: 63  batch:  600  loss: 0.1808\n",
      "epoch: 63  batch:  700  loss: 0.3918\n",
      "epoch: 63  batch:  800  loss: 0.4375\n",
      "epoch: 63  batch:  900  loss: 0.2840\n",
      "epoch: 63  avg train loss: 0.2651  avg train accu: 0.926\n",
      "epoch: 63  avg test  loss: 0.1544  avg test  accu: 0.956\n",
      "==================================================================================\n",
      "epoch: 64  batch:    0  loss: 0.2312\n",
      "epoch: 64  batch:  100  loss: 0.2073\n",
      "epoch: 64  batch:  200  loss: 0.1126\n",
      "epoch: 64  batch:  300  loss: 0.3591\n",
      "epoch: 64  batch:  400  loss: 0.2425\n",
      "epoch: 64  batch:  500  loss: 0.3024\n",
      "epoch: 64  batch:  600  loss: 0.3590\n",
      "epoch: 64  batch:  700  loss: 0.1650\n",
      "epoch: 64  batch:  800  loss: 0.3585\n",
      "epoch: 64  batch:  900  loss: 0.2649\n",
      "epoch: 64  avg train loss: 0.2625  avg train accu: 0.926\n",
      "epoch: 64  avg test  loss: 0.1539  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 65  batch:    0  loss: 0.3324\n",
      "epoch: 65  batch:  100  loss: 0.3066\n",
      "epoch: 65  batch:  200  loss: 0.1892\n",
      "epoch: 65  batch:  300  loss: 0.2499\n",
      "epoch: 65  batch:  400  loss: 0.2148\n",
      "epoch: 65  batch:  500  loss: 0.3108\n",
      "epoch: 65  batch:  600  loss: 0.2535\n",
      "epoch: 65  batch:  700  loss: 0.2474\n",
      "epoch: 65  batch:  800  loss: 0.1977\n",
      "epoch: 65  batch:  900  loss: 0.2345\n",
      "epoch: 65  avg train loss: 0.2622  avg train accu: 0.927\n",
      "epoch: 65  avg test  loss: 0.1537  avg test  accu: 0.956\n",
      "==================================================================================\n",
      "epoch: 66  batch:    0  loss: 0.3044\n",
      "epoch: 66  batch:  100  loss: 0.1977\n",
      "epoch: 66  batch:  200  loss: 0.2514\n",
      "epoch: 66  batch:  300  loss: 0.2655\n",
      "epoch: 66  batch:  400  loss: 0.3774\n",
      "epoch: 66  batch:  500  loss: 0.1779\n",
      "epoch: 66  batch:  600  loss: 0.3186\n",
      "epoch: 66  batch:  700  loss: 0.3754\n",
      "epoch: 66  batch:  800  loss: 0.1805\n",
      "epoch: 66  batch:  900  loss: 0.1835\n",
      "epoch: 66  avg train loss: 0.2612  avg train accu: 0.927\n",
      "epoch: 66  avg test  loss: 0.1535  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 67  batch:    0  loss: 0.1840\n",
      "epoch: 67  batch:  100  loss: 0.2913\n",
      "epoch: 67  batch:  200  loss: 0.2491\n",
      "epoch: 67  batch:  300  loss: 0.3051\n",
      "epoch: 67  batch:  400  loss: 0.3005\n",
      "epoch: 67  batch:  500  loss: 0.2117\n",
      "epoch: 67  batch:  600  loss: 0.1894\n",
      "epoch: 67  batch:  700  loss: 0.2620\n",
      "epoch: 67  batch:  800  loss: 0.2871\n",
      "epoch: 67  batch:  900  loss: 0.3260\n",
      "epoch: 67  avg train loss: 0.2601  avg train accu: 0.927\n",
      "epoch: 67  avg test  loss: 0.1538  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 68  batch:    0  loss: 0.3268\n",
      "epoch: 68  batch:  100  loss: 0.3251\n",
      "epoch: 68  batch:  200  loss: 0.1908\n",
      "epoch: 68  batch:  300  loss: 0.1613\n",
      "epoch: 68  batch:  400  loss: 0.3559\n",
      "epoch: 68  batch:  500  loss: 0.2965\n",
      "epoch: 68  batch:  600  loss: 0.3641\n",
      "epoch: 68  batch:  700  loss: 0.2574\n",
      "epoch: 68  batch:  800  loss: 0.1478\n",
      "epoch: 68  batch:  900  loss: 0.5255\n",
      "epoch: 68  avg train loss: 0.2617  avg train accu: 0.927\n",
      "epoch: 68  avg test  loss: 0.1530  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 69  batch:    0  loss: 0.2767\n",
      "epoch: 69  batch:  100  loss: 0.3983\n",
      "epoch: 69  batch:  200  loss: 0.3904\n",
      "epoch: 69  batch:  300  loss: 0.5112\n",
      "epoch: 69  batch:  400  loss: 0.1667\n",
      "epoch: 69  batch:  500  loss: 0.2157\n",
      "epoch: 69  batch:  600  loss: 0.3557\n",
      "epoch: 69  batch:  700  loss: 0.2739\n",
      "epoch: 69  batch:  800  loss: 0.4159\n",
      "epoch: 69  batch:  900  loss: 0.2529\n",
      "epoch: 69  avg train loss: 0.2613  avg train accu: 0.927\n",
      "epoch: 69  avg test  loss: 0.1530  avg test  accu: 0.956\n",
      "==================================================================================\n",
      "epoch: 70  batch:    0  loss: 0.4858\n",
      "epoch: 70  batch:  100  loss: 0.2372\n",
      "epoch: 70  batch:  200  loss: 0.1799\n",
      "epoch: 70  batch:  300  loss: 0.1652\n",
      "epoch: 70  batch:  400  loss: 0.2574\n",
      "epoch: 70  batch:  500  loss: 0.2321\n",
      "epoch: 70  batch:  600  loss: 0.2014\n",
      "epoch: 70  batch:  700  loss: 0.2071\n",
      "epoch: 70  batch:  800  loss: 0.2141\n",
      "epoch: 70  batch:  900  loss: 0.2429\n",
      "epoch: 70  avg train loss: 0.2608  avg train accu: 0.927\n",
      "epoch: 70  avg test  loss: 0.1517  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 71  batch:    0  loss: 0.3711\n",
      "epoch: 71  batch:  100  loss: 0.3714\n",
      "epoch: 71  batch:  200  loss: 0.3538\n",
      "epoch: 71  batch:  300  loss: 0.4937\n",
      "epoch: 71  batch:  400  loss: 0.5757\n",
      "epoch: 71  batch:  500  loss: 0.3497\n",
      "epoch: 71  batch:  600  loss: 0.1843\n",
      "epoch: 71  batch:  700  loss: 0.3336\n",
      "epoch: 71  batch:  800  loss: 0.1190\n",
      "epoch: 71  batch:  900  loss: 0.2264\n",
      "epoch: 71  avg train loss: 0.2598  avg train accu: 0.928\n",
      "epoch: 71  avg test  loss: 0.1527  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 72  batch:    0  loss: 0.2106\n",
      "epoch: 72  batch:  100  loss: 0.2460\n",
      "epoch: 72  batch:  200  loss: 0.2375\n",
      "epoch: 72  batch:  300  loss: 0.3083\n",
      "epoch: 72  batch:  400  loss: 0.3141\n",
      "epoch: 72  batch:  500  loss: 0.3059\n",
      "epoch: 72  batch:  600  loss: 0.3083\n",
      "epoch: 72  batch:  700  loss: 0.1318\n",
      "epoch: 72  batch:  800  loss: 0.1939\n",
      "epoch: 72  batch:  900  loss: 0.2517\n",
      "epoch: 72  avg train loss: 0.2583  avg train accu: 0.928\n",
      "epoch: 72  avg test  loss: 0.1518  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 73  batch:    0  loss: 0.3346\n",
      "epoch: 73  batch:  100  loss: 0.3337\n",
      "epoch: 73  batch:  200  loss: 0.1726\n",
      "epoch: 73  batch:  300  loss: 0.1577\n",
      "epoch: 73  batch:  400  loss: 0.1787\n",
      "epoch: 73  batch:  500  loss: 0.1934\n",
      "epoch: 73  batch:  600  loss: 0.2303\n",
      "epoch: 73  batch:  700  loss: 0.2585\n",
      "epoch: 73  batch:  800  loss: 0.3514\n",
      "epoch: 73  batch:  900  loss: 0.4207\n",
      "epoch: 73  avg train loss: 0.2604  avg train accu: 0.927\n",
      "epoch: 73  avg test  loss: 0.1521  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 74  batch:    0  loss: 0.2087\n",
      "epoch: 74  batch:  100  loss: 0.2640\n",
      "epoch: 74  batch:  200  loss: 0.1420\n",
      "epoch: 74  batch:  300  loss: 0.2584\n",
      "epoch: 74  batch:  400  loss: 0.1519\n",
      "epoch: 74  batch:  500  loss: 0.2408\n",
      "epoch: 74  batch:  600  loss: 0.3764\n",
      "epoch: 74  batch:  700  loss: 0.2045\n",
      "epoch: 74  batch:  800  loss: 0.1817\n",
      "epoch: 74  batch:  900  loss: 0.2864\n",
      "epoch: 74  avg train loss: 0.2584  avg train accu: 0.928\n",
      "epoch: 74  avg test  loss: 0.1504  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 75  batch:    0  loss: 0.1221\n",
      "epoch: 75  batch:  100  loss: 0.2253\n",
      "epoch: 75  batch:  200  loss: 0.1457\n",
      "epoch: 75  batch:  300  loss: 0.1529\n",
      "epoch: 75  batch:  400  loss: 0.2830\n",
      "epoch: 75  batch:  500  loss: 0.2988\n",
      "epoch: 75  batch:  600  loss: 0.3669\n",
      "epoch: 75  batch:  700  loss: 0.1847\n",
      "epoch: 75  batch:  800  loss: 0.2458\n",
      "epoch: 75  batch:  900  loss: 0.1806\n",
      "epoch: 75  avg train loss: 0.2566  avg train accu: 0.926\n",
      "epoch: 75  avg test  loss: 0.1512  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 76  batch:    0  loss: 0.2513\n",
      "epoch: 76  batch:  100  loss: 0.2885\n",
      "epoch: 76  batch:  200  loss: 0.2003\n",
      "epoch: 76  batch:  300  loss: 0.2598\n",
      "epoch: 76  batch:  400  loss: 0.1864\n",
      "epoch: 76  batch:  500  loss: 0.1661\n",
      "epoch: 76  batch:  600  loss: 0.1879\n",
      "epoch: 76  batch:  700  loss: 0.2245\n",
      "epoch: 76  batch:  800  loss: 0.1874\n",
      "epoch: 76  batch:  900  loss: 0.4650\n",
      "epoch: 76  avg train loss: 0.2574  avg train accu: 0.927\n",
      "epoch: 76  avg test  loss: 0.1515  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 77  batch:    0  loss: 0.1333\n",
      "epoch: 77  batch:  100  loss: 0.1414\n",
      "epoch: 77  batch:  200  loss: 0.3081\n",
      "epoch: 77  batch:  300  loss: 0.1505\n",
      "epoch: 77  batch:  400  loss: 0.1110\n",
      "epoch: 77  batch:  500  loss: 0.2013\n",
      "epoch: 77  batch:  600  loss: 0.2870\n",
      "epoch: 77  batch:  700  loss: 0.3002\n",
      "epoch: 77  batch:  800  loss: 0.2655\n",
      "epoch: 77  batch:  900  loss: 0.3333\n",
      "epoch: 77  avg train loss: 0.2575  avg train accu: 0.927\n",
      "epoch: 77  avg test  loss: 0.1503  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 78  batch:    0  loss: 0.1455\n",
      "epoch: 78  batch:  100  loss: 0.3223\n",
      "epoch: 78  batch:  200  loss: 0.2163\n",
      "epoch: 78  batch:  300  loss: 0.2319\n",
      "epoch: 78  batch:  400  loss: 0.2289\n",
      "epoch: 78  batch:  500  loss: 0.1485\n",
      "epoch: 78  batch:  600  loss: 0.3901\n",
      "epoch: 78  batch:  700  loss: 0.2724\n",
      "epoch: 78  batch:  800  loss: 0.2534\n",
      "epoch: 78  batch:  900  loss: 0.1949\n",
      "epoch: 78  avg train loss: 0.2580  avg train accu: 0.929\n",
      "epoch: 78  avg test  loss: 0.1503  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 79  batch:    0  loss: 0.5623\n",
      "epoch: 79  batch:  100  loss: 0.1538\n",
      "epoch: 79  batch:  200  loss: 0.3049\n",
      "epoch: 79  batch:  300  loss: 0.2978\n",
      "epoch: 79  batch:  400  loss: 0.3657\n",
      "epoch: 79  batch:  500  loss: 0.4537\n",
      "epoch: 79  batch:  600  loss: 0.1908\n",
      "epoch: 79  batch:  700  loss: 0.1421\n",
      "epoch: 79  batch:  800  loss: 0.3151\n",
      "epoch: 79  batch:  900  loss: 0.1546\n",
      "epoch: 79  avg train loss: 0.2542  avg train accu: 0.928\n",
      "epoch: 79  avg test  loss: 0.1489  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 80  batch:    0  loss: 0.2003\n",
      "epoch: 80  batch:  100  loss: 0.2856\n",
      "epoch: 80  batch:  200  loss: 0.1725\n",
      "epoch: 80  batch:  300  loss: 0.3550\n",
      "epoch: 80  batch:  400  loss: 0.1817\n",
      "epoch: 80  batch:  500  loss: 0.2845\n",
      "epoch: 80  batch:  600  loss: 0.2637\n",
      "epoch: 80  batch:  700  loss: 0.2449\n",
      "epoch: 80  batch:  800  loss: 0.3286\n",
      "epoch: 80  batch:  900  loss: 0.1845\n",
      "epoch: 80  avg train loss: 0.2587  avg train accu: 0.927\n",
      "epoch: 80  avg test  loss: 0.1499  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 81  batch:    0  loss: 0.3445\n",
      "epoch: 81  batch:  100  loss: 0.1608\n",
      "epoch: 81  batch:  200  loss: 0.1282\n",
      "epoch: 81  batch:  300  loss: 0.2367\n",
      "epoch: 81  batch:  400  loss: 0.2276\n",
      "epoch: 81  batch:  500  loss: 0.2067\n",
      "epoch: 81  batch:  600  loss: 0.3878\n",
      "epoch: 81  batch:  700  loss: 0.2459\n",
      "epoch: 81  batch:  800  loss: 0.2110\n",
      "epoch: 81  batch:  900  loss: 0.1641\n",
      "epoch: 81  avg train loss: 0.2565  avg train accu: 0.928\n",
      "epoch: 81  avg test  loss: 0.1505  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 82  batch:    0  loss: 0.2884\n",
      "epoch: 82  batch:  100  loss: 0.3535\n",
      "epoch: 82  batch:  200  loss: 0.1516\n",
      "epoch: 82  batch:  300  loss: 0.3920\n",
      "epoch: 82  batch:  400  loss: 0.2562\n",
      "epoch: 82  batch:  500  loss: 0.5942\n",
      "epoch: 82  batch:  600  loss: 0.1986\n",
      "epoch: 82  batch:  700  loss: 0.2125\n",
      "epoch: 82  batch:  800  loss: 0.2543\n",
      "epoch: 82  batch:  900  loss: 0.2039\n",
      "epoch: 82  avg train loss: 0.2553  avg train accu: 0.928\n",
      "epoch: 82  avg test  loss: 0.1501  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 83  batch:    0  loss: 0.4292\n",
      "epoch: 83  batch:  100  loss: 0.1418\n",
      "epoch: 83  batch:  200  loss: 0.1497\n",
      "epoch: 83  batch:  300  loss: 0.1558\n",
      "epoch: 83  batch:  400  loss: 0.2212\n",
      "epoch: 83  batch:  500  loss: 0.2096\n",
      "epoch: 83  batch:  600  loss: 0.2551\n",
      "epoch: 83  batch:  700  loss: 0.3477\n",
      "epoch: 83  batch:  800  loss: 0.2922\n",
      "epoch: 83  batch:  900  loss: 0.1308\n",
      "epoch: 83  avg train loss: 0.2562  avg train accu: 0.928\n",
      "epoch: 83  avg test  loss: 0.1494  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 84  batch:    0  loss: 0.2066\n",
      "epoch: 84  batch:  100  loss: 0.2452\n",
      "epoch: 84  batch:  200  loss: 0.2978\n",
      "epoch: 84  batch:  300  loss: 0.1878\n",
      "epoch: 84  batch:  400  loss: 0.1937\n",
      "epoch: 84  batch:  500  loss: 0.1719\n",
      "epoch: 84  batch:  600  loss: 0.3117\n",
      "epoch: 84  batch:  700  loss: 0.3402\n",
      "epoch: 84  batch:  800  loss: 0.2618\n",
      "epoch: 84  batch:  900  loss: 0.2707\n",
      "epoch: 84  avg train loss: 0.2556  avg train accu: 0.929\n",
      "epoch: 84  avg test  loss: 0.1498  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 85  batch:    0  loss: 0.2598\n",
      "epoch: 85  batch:  100  loss: 0.2050\n",
      "epoch: 85  batch:  200  loss: 0.2250\n",
      "epoch: 85  batch:  300  loss: 0.2958\n",
      "epoch: 85  batch:  400  loss: 0.2617\n",
      "epoch: 85  batch:  500  loss: 0.3242\n",
      "epoch: 85  batch:  600  loss: 0.3735\n",
      "epoch: 85  batch:  700  loss: 0.1758\n",
      "epoch: 85  batch:  800  loss: 0.2226\n",
      "epoch: 85  batch:  900  loss: 0.2460\n",
      "epoch: 85  avg train loss: 0.2557  avg train accu: 0.929\n",
      "epoch: 85  avg test  loss: 0.1487  avg test  accu: 0.957\n",
      "==================================================================================\n",
      "epoch: 86  batch:    0  loss: 0.2043\n",
      "epoch: 86  batch:  100  loss: 0.2227\n",
      "epoch: 86  batch:  200  loss: 0.2210\n",
      "epoch: 86  batch:  300  loss: 0.2406\n",
      "epoch: 86  batch:  400  loss: 0.2647\n",
      "epoch: 86  batch:  500  loss: 0.2561\n",
      "epoch: 86  batch:  600  loss: 0.2578\n",
      "epoch: 86  batch:  700  loss: 0.2702\n",
      "epoch: 86  batch:  800  loss: 0.3322\n",
      "epoch: 86  batch:  900  loss: 0.2203\n",
      "epoch: 86  avg train loss: 0.2546  avg train accu: 0.929\n",
      "epoch: 86  avg test  loss: 0.1481  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 87  batch:    0  loss: 0.1818\n",
      "epoch: 87  batch:  100  loss: 0.2662\n",
      "epoch: 87  batch:  200  loss: 0.2413\n",
      "epoch: 87  batch:  300  loss: 0.3567\n",
      "epoch: 87  batch:  400  loss: 0.2321\n",
      "epoch: 87  batch:  500  loss: 0.2390\n",
      "epoch: 87  batch:  600  loss: 0.1415\n",
      "epoch: 87  batch:  700  loss: 0.1806\n",
      "epoch: 87  batch:  800  loss: 0.2614\n",
      "epoch: 87  batch:  900  loss: 0.2150\n",
      "epoch: 87  avg train loss: 0.2559  avg train accu: 0.928\n",
      "epoch: 87  avg test  loss: 0.1478  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 88  batch:    0  loss: 0.2544\n",
      "epoch: 88  batch:  100  loss: 0.2785\n",
      "epoch: 88  batch:  200  loss: 0.3944\n",
      "epoch: 88  batch:  300  loss: 0.2527\n",
      "epoch: 88  batch:  400  loss: 0.3449\n",
      "epoch: 88  batch:  500  loss: 0.3382\n",
      "epoch: 88  batch:  600  loss: 0.3121\n",
      "epoch: 88  batch:  700  loss: 0.3647\n",
      "epoch: 88  batch:  800  loss: 0.2549\n",
      "epoch: 88  batch:  900  loss: 0.2000\n",
      "epoch: 88  avg train loss: 0.2560  avg train accu: 0.927\n",
      "epoch: 88  avg test  loss: 0.1477  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 89  batch:    0  loss: 0.3577\n",
      "epoch: 89  batch:  100  loss: 0.2239\n",
      "epoch: 89  batch:  200  loss: 0.2138\n",
      "epoch: 89  batch:  300  loss: 0.2009\n",
      "epoch: 89  batch:  400  loss: 0.2851\n",
      "epoch: 89  batch:  500  loss: 0.1313\n",
      "epoch: 89  batch:  600  loss: 0.3544\n",
      "epoch: 89  batch:  700  loss: 0.2211\n",
      "epoch: 89  batch:  800  loss: 0.2283\n",
      "epoch: 89  batch:  900  loss: 0.1447\n",
      "epoch: 89  avg train loss: 0.2547  avg train accu: 0.929\n",
      "epoch: 89  avg test  loss: 0.1484  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 90  batch:    0  loss: 0.1991\n",
      "epoch: 90  batch:  100  loss: 0.2280\n",
      "epoch: 90  batch:  200  loss: 0.3412\n",
      "epoch: 90  batch:  300  loss: 0.1930\n",
      "epoch: 90  batch:  400  loss: 0.2229\n",
      "epoch: 90  batch:  500  loss: 0.2277\n",
      "epoch: 90  batch:  600  loss: 0.2178\n",
      "epoch: 90  batch:  700  loss: 0.1853\n",
      "epoch: 90  batch:  800  loss: 0.3713\n",
      "epoch: 90  batch:  900  loss: 0.2382\n",
      "epoch: 90  avg train loss: 0.2553  avg train accu: 0.929\n",
      "epoch: 90  avg test  loss: 0.1481  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 91  batch:    0  loss: 0.2548\n",
      "epoch: 91  batch:  100  loss: 0.2093\n",
      "epoch: 91  batch:  200  loss: 0.2061\n",
      "epoch: 91  batch:  300  loss: 0.1593\n",
      "epoch: 91  batch:  400  loss: 0.1391\n",
      "epoch: 91  batch:  500  loss: 0.2142\n",
      "epoch: 91  batch:  600  loss: 0.3563\n",
      "epoch: 91  batch:  700  loss: 0.2519\n",
      "epoch: 91  batch:  800  loss: 0.3784\n",
      "epoch: 91  batch:  900  loss: 0.2839\n",
      "epoch: 91  avg train loss: 0.2537  avg train accu: 0.928\n",
      "epoch: 91  avg test  loss: 0.1482  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 92  batch:    0  loss: 0.1945\n",
      "epoch: 92  batch:  100  loss: 0.1621\n",
      "epoch: 92  batch:  200  loss: 0.2039\n",
      "epoch: 92  batch:  300  loss: 0.2755\n",
      "epoch: 92  batch:  400  loss: 0.3249\n",
      "epoch: 92  batch:  500  loss: 0.3361\n",
      "epoch: 92  batch:  600  loss: 0.4647\n",
      "epoch: 92  batch:  700  loss: 0.2102\n",
      "epoch: 92  batch:  800  loss: 0.2426\n",
      "epoch: 92  batch:  900  loss: 0.2237\n",
      "epoch: 92  avg train loss: 0.2539  avg train accu: 0.927\n",
      "epoch: 92  avg test  loss: 0.1492  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 93  batch:    0  loss: 0.2748\n",
      "epoch: 93  batch:  100  loss: 0.1926\n",
      "epoch: 93  batch:  200  loss: 0.1692\n",
      "epoch: 93  batch:  300  loss: 0.2165\n",
      "epoch: 93  batch:  400  loss: 0.1360\n",
      "epoch: 93  batch:  500  loss: 0.3987\n",
      "epoch: 93  batch:  600  loss: 0.1574\n",
      "epoch: 93  batch:  700  loss: 0.2435\n",
      "epoch: 93  batch:  800  loss: 0.2056\n",
      "epoch: 93  batch:  900  loss: 0.0934\n",
      "epoch: 93  avg train loss: 0.2514  avg train accu: 0.930\n",
      "epoch: 93  avg test  loss: 0.1468  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 94  batch:    0  loss: 0.3426\n",
      "epoch: 94  batch:  100  loss: 0.1902\n",
      "epoch: 94  batch:  200  loss: 0.1796\n",
      "epoch: 94  batch:  300  loss: 0.3141\n",
      "epoch: 94  batch:  400  loss: 0.2200\n",
      "epoch: 94  batch:  500  loss: 0.2329\n",
      "epoch: 94  batch:  600  loss: 0.1862\n",
      "epoch: 94  batch:  700  loss: 0.1937\n",
      "epoch: 94  batch:  800  loss: 0.3805\n",
      "epoch: 94  batch:  900  loss: 0.1789\n",
      "epoch: 94  avg train loss: 0.2536  avg train accu: 0.929\n",
      "epoch: 94  avg test  loss: 0.1482  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 95  batch:    0  loss: 0.2229\n",
      "epoch: 95  batch:  100  loss: 0.3280\n",
      "epoch: 95  batch:  200  loss: 0.2198\n",
      "epoch: 95  batch:  300  loss: 0.3411\n",
      "epoch: 95  batch:  400  loss: 0.2212\n",
      "epoch: 95  batch:  500  loss: 0.1317\n",
      "epoch: 95  batch:  600  loss: 0.2123\n",
      "epoch: 95  batch:  700  loss: 0.1758\n",
      "epoch: 95  batch:  800  loss: 0.4215\n",
      "epoch: 95  batch:  900  loss: 0.2605\n",
      "epoch: 95  avg train loss: 0.2555  avg train accu: 0.927\n",
      "epoch: 95  avg test  loss: 0.1468  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 96  batch:    0  loss: 0.1350\n",
      "epoch: 96  batch:  100  loss: 0.3182\n",
      "epoch: 96  batch:  200  loss: 0.1844\n",
      "epoch: 96  batch:  300  loss: 0.2915\n",
      "epoch: 96  batch:  400  loss: 0.2761\n",
      "epoch: 96  batch:  500  loss: 0.1367\n",
      "epoch: 96  batch:  600  loss: 0.1851\n",
      "epoch: 96  batch:  700  loss: 0.1916\n",
      "epoch: 96  batch:  800  loss: 0.0854\n",
      "epoch: 96  batch:  900  loss: 0.1928\n",
      "epoch: 96  avg train loss: 0.2546  avg train accu: 0.928\n",
      "epoch: 96  avg test  loss: 0.1467  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 97  batch:    0  loss: 0.2646\n",
      "epoch: 97  batch:  100  loss: 0.2008\n",
      "epoch: 97  batch:  200  loss: 0.1755\n",
      "epoch: 97  batch:  300  loss: 0.3488\n",
      "epoch: 97  batch:  400  loss: 0.3288\n",
      "epoch: 97  batch:  500  loss: 0.5126\n",
      "epoch: 97  batch:  600  loss: 0.2363\n",
      "epoch: 97  batch:  700  loss: 0.1876\n",
      "epoch: 97  batch:  800  loss: 0.3597\n",
      "epoch: 97  batch:  900  loss: 0.1891\n",
      "epoch: 97  avg train loss: 0.2538  avg train accu: 0.928\n",
      "epoch: 97  avg test  loss: 0.1468  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 98  batch:    0  loss: 0.2372\n",
      "epoch: 98  batch:  100  loss: 0.0932\n",
      "epoch: 98  batch:  200  loss: 0.2112\n",
      "epoch: 98  batch:  300  loss: 0.3190\n",
      "epoch: 98  batch:  400  loss: 0.1888\n",
      "epoch: 98  batch:  500  loss: 0.3002\n",
      "epoch: 98  batch:  600  loss: 0.2816\n",
      "epoch: 98  batch:  700  loss: 0.2522\n",
      "epoch: 98  batch:  800  loss: 0.2039\n",
      "epoch: 98  batch:  900  loss: 0.2116\n",
      "epoch: 98  avg train loss: 0.2543  avg train accu: 0.929\n",
      "epoch: 98  avg test  loss: 0.1477  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 99  batch:    0  loss: 0.1804\n",
      "epoch: 99  batch:  100  loss: 0.3403\n",
      "epoch: 99  batch:  200  loss: 0.1542\n",
      "epoch: 99  batch:  300  loss: 0.2465\n",
      "epoch: 99  batch:  400  loss: 0.3098\n",
      "epoch: 99  batch:  500  loss: 0.2858\n",
      "epoch: 99  batch:  600  loss: 0.2778\n",
      "epoch: 99  batch:  700  loss: 0.1715\n",
      "epoch: 99  batch:  800  loss: 0.2343\n",
      "epoch: 99  batch:  900  loss: 0.2947\n",
      "epoch: 99  avg train loss: 0.2522  avg train accu: 0.929\n",
      "epoch: 99  avg test  loss: 0.1467  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 100  batch:    0  loss: 0.3857\n",
      "epoch: 100  batch:  100  loss: 0.1853\n",
      "epoch: 100  batch:  200  loss: 0.2825\n",
      "epoch: 100  batch:  300  loss: 0.2212\n",
      "epoch: 100  batch:  400  loss: 0.5263\n",
      "epoch: 100  batch:  500  loss: 0.3713\n",
      "epoch: 100  batch:  600  loss: 0.2974\n",
      "epoch: 100  batch:  700  loss: 0.3081\n",
      "epoch: 100  batch:  800  loss: 0.2957\n",
      "epoch: 100  batch:  900  loss: 0.2741\n",
      "epoch: 100  avg train loss: 0.2518  avg train accu: 0.930\n",
      "epoch: 100  avg test  loss: 0.1473  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 101  batch:    0  loss: 0.1359\n",
      "epoch: 101  batch:  100  loss: 0.2273\n",
      "epoch: 101  batch:  200  loss: 0.2301\n",
      "epoch: 101  batch:  300  loss: 0.2363\n",
      "epoch: 101  batch:  400  loss: 0.2280\n",
      "epoch: 101  batch:  500  loss: 0.1885\n",
      "epoch: 101  batch:  600  loss: 0.1435\n",
      "epoch: 101  batch:  700  loss: 0.2442\n",
      "epoch: 101  batch:  800  loss: 0.2093\n",
      "epoch: 101  batch:  900  loss: 0.3250\n",
      "epoch: 101  avg train loss: 0.2520  avg train accu: 0.929\n",
      "epoch: 101  avg test  loss: 0.1470  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 102  batch:    0  loss: 0.2200\n",
      "epoch: 102  batch:  100  loss: 0.1977\n",
      "epoch: 102  batch:  200  loss: 0.2139\n",
      "epoch: 102  batch:  300  loss: 0.2221\n",
      "epoch: 102  batch:  400  loss: 0.2811\n",
      "epoch: 102  batch:  500  loss: 0.4078\n",
      "epoch: 102  batch:  600  loss: 0.1705\n",
      "epoch: 102  batch:  700  loss: 0.1734\n",
      "epoch: 102  batch:  800  loss: 0.1642\n",
      "epoch: 102  batch:  900  loss: 0.1666\n",
      "epoch: 102  avg train loss: 0.2529  avg train accu: 0.928\n",
      "epoch: 102  avg test  loss: 0.1474  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 103  batch:    0  loss: 0.3391\n",
      "epoch: 103  batch:  100  loss: 0.4182\n",
      "epoch: 103  batch:  200  loss: 0.2590\n",
      "epoch: 103  batch:  300  loss: 0.2182\n",
      "epoch: 103  batch:  400  loss: 0.1910\n",
      "epoch: 103  batch:  500  loss: 0.2725\n",
      "epoch: 103  batch:  600  loss: 0.3328\n",
      "epoch: 103  batch:  700  loss: 0.2524\n",
      "epoch: 103  batch:  800  loss: 0.2310\n",
      "epoch: 103  batch:  900  loss: 0.2815\n",
      "epoch: 103  avg train loss: 0.2499  avg train accu: 0.929\n",
      "epoch: 103  avg test  loss: 0.1471  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 104  batch:    0  loss: 0.1612\n",
      "epoch: 104  batch:  100  loss: 0.2099\n",
      "epoch: 104  batch:  200  loss: 0.1441\n",
      "epoch: 104  batch:  300  loss: 0.2208\n",
      "epoch: 104  batch:  400  loss: 0.4327\n",
      "epoch: 104  batch:  500  loss: 0.3060\n",
      "epoch: 104  batch:  600  loss: 0.2709\n",
      "epoch: 104  batch:  700  loss: 0.2356\n",
      "epoch: 104  batch:  800  loss: 0.3100\n",
      "epoch: 104  batch:  900  loss: 0.2874\n",
      "epoch: 104  avg train loss: 0.2545  avg train accu: 0.929\n",
      "epoch: 104  avg test  loss: 0.1457  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 105  batch:    0  loss: 0.2404\n",
      "epoch: 105  batch:  100  loss: 0.2076\n",
      "epoch: 105  batch:  200  loss: 0.2673\n",
      "epoch: 105  batch:  300  loss: 0.3449\n",
      "epoch: 105  batch:  400  loss: 0.3024\n",
      "epoch: 105  batch:  500  loss: 0.2186\n",
      "epoch: 105  batch:  600  loss: 0.4903\n",
      "epoch: 105  batch:  700  loss: 0.1291\n",
      "epoch: 105  batch:  800  loss: 0.2650\n",
      "epoch: 105  batch:  900  loss: 0.2973\n",
      "epoch: 105  avg train loss: 0.2521  avg train accu: 0.929\n",
      "epoch: 105  avg test  loss: 0.1457  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 106  batch:    0  loss: 0.1518\n",
      "epoch: 106  batch:  100  loss: 0.2268\n",
      "epoch: 106  batch:  200  loss: 0.2708\n",
      "epoch: 106  batch:  300  loss: 0.2391\n",
      "epoch: 106  batch:  400  loss: 0.3545\n",
      "epoch: 106  batch:  500  loss: 0.1427\n",
      "epoch: 106  batch:  600  loss: 0.1974\n",
      "epoch: 106  batch:  700  loss: 0.4674\n",
      "epoch: 106  batch:  800  loss: 0.2060\n",
      "epoch: 106  batch:  900  loss: 0.1495\n",
      "epoch: 106  avg train loss: 0.2520  avg train accu: 0.928\n",
      "epoch: 106  avg test  loss: 0.1470  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 107  batch:    0  loss: 0.1817\n",
      "epoch: 107  batch:  100  loss: 0.2056\n",
      "epoch: 107  batch:  200  loss: 0.2156\n",
      "epoch: 107  batch:  300  loss: 0.2452\n",
      "epoch: 107  batch:  400  loss: 0.1238\n",
      "epoch: 107  batch:  500  loss: 0.4227\n",
      "epoch: 107  batch:  600  loss: 0.2904\n",
      "epoch: 107  batch:  700  loss: 0.2006\n",
      "epoch: 107  batch:  800  loss: 0.4623\n",
      "epoch: 107  batch:  900  loss: 0.2843\n",
      "epoch: 107  avg train loss: 0.2507  avg train accu: 0.929\n",
      "epoch: 107  avg test  loss: 0.1459  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 108  batch:    0  loss: 0.2646\n",
      "epoch: 108  batch:  100  loss: 0.2558\n",
      "epoch: 108  batch:  200  loss: 0.1522\n",
      "epoch: 108  batch:  300  loss: 0.4483\n",
      "epoch: 108  batch:  400  loss: 0.2561\n",
      "epoch: 108  batch:  500  loss: 0.1463\n",
      "epoch: 108  batch:  600  loss: 0.2195\n",
      "epoch: 108  batch:  700  loss: 0.4783\n",
      "epoch: 108  batch:  800  loss: 0.2751\n",
      "epoch: 108  batch:  900  loss: 0.2516\n",
      "epoch: 108  avg train loss: 0.2513  avg train accu: 0.930\n",
      "epoch: 108  avg test  loss: 0.1455  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 109  batch:    0  loss: 0.1384\n",
      "epoch: 109  batch:  100  loss: 0.2154\n",
      "epoch: 109  batch:  200  loss: 0.1298\n",
      "epoch: 109  batch:  300  loss: 0.1948\n",
      "epoch: 109  batch:  400  loss: 0.2512\n",
      "epoch: 109  batch:  500  loss: 0.2750\n",
      "epoch: 109  batch:  600  loss: 0.2115\n",
      "epoch: 109  batch:  700  loss: 0.2536\n",
      "epoch: 109  batch:  800  loss: 0.3304\n",
      "epoch: 109  batch:  900  loss: 0.2616\n",
      "epoch: 109  avg train loss: 0.2526  avg train accu: 0.928\n",
      "epoch: 109  avg test  loss: 0.1460  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 110  batch:    0  loss: 0.3026\n",
      "epoch: 110  batch:  100  loss: 0.1425\n",
      "epoch: 110  batch:  200  loss: 0.1814\n",
      "epoch: 110  batch:  300  loss: 0.4302\n",
      "epoch: 110  batch:  400  loss: 0.2878\n",
      "epoch: 110  batch:  500  loss: 0.2570\n",
      "epoch: 110  batch:  600  loss: 0.1716\n",
      "epoch: 110  batch:  700  loss: 0.5882\n",
      "epoch: 110  batch:  800  loss: 0.2022\n",
      "epoch: 110  batch:  900  loss: 0.2880\n",
      "epoch: 110  avg train loss: 0.2515  avg train accu: 0.929\n",
      "epoch: 110  avg test  loss: 0.1456  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 111  batch:    0  loss: 0.1879\n",
      "epoch: 111  batch:  100  loss: 0.2544\n",
      "epoch: 111  batch:  200  loss: 0.4418\n",
      "epoch: 111  batch:  300  loss: 0.2394\n",
      "epoch: 111  batch:  400  loss: 0.1350\n",
      "epoch: 111  batch:  500  loss: 0.3730\n",
      "epoch: 111  batch:  600  loss: 0.2593\n",
      "epoch: 111  batch:  700  loss: 0.2629\n",
      "epoch: 111  batch:  800  loss: 0.2191\n",
      "epoch: 111  batch:  900  loss: 0.4103\n",
      "epoch: 111  avg train loss: 0.2521  avg train accu: 0.929\n",
      "epoch: 111  avg test  loss: 0.1462  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 112  batch:    0  loss: 0.1539\n",
      "epoch: 112  batch:  100  loss: 0.3379\n",
      "epoch: 112  batch:  200  loss: 0.2459\n",
      "epoch: 112  batch:  300  loss: 0.3046\n",
      "epoch: 112  batch:  400  loss: 0.2806\n",
      "epoch: 112  batch:  500  loss: 0.2702\n",
      "epoch: 112  batch:  600  loss: 0.2324\n",
      "epoch: 112  batch:  700  loss: 0.1074\n",
      "epoch: 112  batch:  800  loss: 0.1766\n",
      "epoch: 112  batch:  900  loss: 0.1684\n",
      "epoch: 112  avg train loss: 0.2491  avg train accu: 0.930\n",
      "epoch: 112  avg test  loss: 0.1451  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 113  batch:    0  loss: 0.2042\n",
      "epoch: 113  batch:  100  loss: 0.1563\n",
      "epoch: 113  batch:  200  loss: 0.2331\n",
      "epoch: 113  batch:  300  loss: 0.2069\n",
      "epoch: 113  batch:  400  loss: 0.4161\n",
      "epoch: 113  batch:  500  loss: 0.1789\n",
      "epoch: 113  batch:  600  loss: 0.2217\n",
      "epoch: 113  batch:  700  loss: 0.3025\n",
      "epoch: 113  batch:  800  loss: 0.3312\n",
      "epoch: 113  batch:  900  loss: 0.2082\n",
      "epoch: 113  avg train loss: 0.2492  avg train accu: 0.929\n",
      "epoch: 113  avg test  loss: 0.1459  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 114  batch:    0  loss: 0.2778\n",
      "epoch: 114  batch:  100  loss: 0.1834\n",
      "epoch: 114  batch:  200  loss: 0.3714\n",
      "epoch: 114  batch:  300  loss: 0.3300\n",
      "epoch: 114  batch:  400  loss: 0.2901\n",
      "epoch: 114  batch:  500  loss: 0.2817\n",
      "epoch: 114  batch:  600  loss: 0.1974\n",
      "epoch: 114  batch:  700  loss: 0.2752\n",
      "epoch: 114  batch:  800  loss: 0.2296\n",
      "epoch: 114  batch:  900  loss: 0.3426\n",
      "epoch: 114  avg train loss: 0.2502  avg train accu: 0.929\n",
      "epoch: 114  avg test  loss: 0.1459  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 115  batch:    0  loss: 0.2166\n",
      "epoch: 115  batch:  100  loss: 0.2259\n",
      "epoch: 115  batch:  200  loss: 0.2818\n",
      "epoch: 115  batch:  300  loss: 0.1415\n",
      "epoch: 115  batch:  400  loss: 0.2550\n",
      "epoch: 115  batch:  500  loss: 0.2234\n",
      "epoch: 115  batch:  600  loss: 0.3633\n",
      "epoch: 115  batch:  700  loss: 0.1872\n",
      "epoch: 115  batch:  800  loss: 0.2276\n",
      "epoch: 115  batch:  900  loss: 0.2079\n",
      "epoch: 115  avg train loss: 0.2509  avg train accu: 0.928\n",
      "epoch: 115  avg test  loss: 0.1448  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 116  batch:    0  loss: 0.3265\n",
      "epoch: 116  batch:  100  loss: 0.4332\n",
      "epoch: 116  batch:  200  loss: 0.1842\n",
      "epoch: 116  batch:  300  loss: 0.1980\n",
      "epoch: 116  batch:  400  loss: 0.2990\n",
      "epoch: 116  batch:  500  loss: 0.1781\n",
      "epoch: 116  batch:  600  loss: 0.3102\n",
      "epoch: 116  batch:  700  loss: 0.2222\n",
      "epoch: 116  batch:  800  loss: 0.4026\n",
      "epoch: 116  batch:  900  loss: 0.1493\n",
      "epoch: 116  avg train loss: 0.2504  avg train accu: 0.930\n",
      "epoch: 116  avg test  loss: 0.1453  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 117  batch:    0  loss: 0.3772\n",
      "epoch: 117  batch:  100  loss: 0.1856\n",
      "epoch: 117  batch:  200  loss: 0.2460\n",
      "epoch: 117  batch:  300  loss: 0.7257\n",
      "epoch: 117  batch:  400  loss: 0.2811\n",
      "epoch: 117  batch:  500  loss: 0.4325\n",
      "epoch: 117  batch:  600  loss: 0.2837\n",
      "epoch: 117  batch:  700  loss: 0.2416\n",
      "epoch: 117  batch:  800  loss: 0.2665\n",
      "epoch: 117  batch:  900  loss: 0.1884\n",
      "epoch: 117  avg train loss: 0.2483  avg train accu: 0.930\n",
      "epoch: 117  avg test  loss: 0.1450  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 118  batch:    0  loss: 0.1572\n",
      "epoch: 118  batch:  100  loss: 0.1668\n",
      "epoch: 118  batch:  200  loss: 0.2329\n",
      "epoch: 118  batch:  300  loss: 0.2830\n",
      "epoch: 118  batch:  400  loss: 0.2495\n",
      "epoch: 118  batch:  500  loss: 0.0981\n",
      "epoch: 118  batch:  600  loss: 0.2158\n",
      "epoch: 118  batch:  700  loss: 0.1522\n",
      "epoch: 118  batch:  800  loss: 0.3295\n",
      "epoch: 118  batch:  900  loss: 0.1921\n",
      "epoch: 118  avg train loss: 0.2512  avg train accu: 0.930\n",
      "epoch: 118  avg test  loss: 0.1449  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 119  batch:    0  loss: 0.1985\n",
      "epoch: 119  batch:  100  loss: 0.2088\n",
      "epoch: 119  batch:  200  loss: 0.2028\n",
      "epoch: 119  batch:  300  loss: 0.3180\n",
      "epoch: 119  batch:  400  loss: 0.2000\n",
      "epoch: 119  batch:  500  loss: 0.4432\n",
      "epoch: 119  batch:  600  loss: 0.2391\n",
      "epoch: 119  batch:  700  loss: 0.3327\n",
      "epoch: 119  batch:  800  loss: 0.1415\n",
      "epoch: 119  batch:  900  loss: 0.1564\n",
      "epoch: 119  avg train loss: 0.2494  avg train accu: 0.929\n",
      "epoch: 119  avg test  loss: 0.1459  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 120  batch:    0  loss: 0.2457\n",
      "epoch: 120  batch:  100  loss: 0.2446\n",
      "epoch: 120  batch:  200  loss: 0.1344\n",
      "epoch: 120  batch:  300  loss: 0.1725\n",
      "epoch: 120  batch:  400  loss: 0.2095\n",
      "epoch: 120  batch:  500  loss: 0.0983\n",
      "epoch: 120  batch:  600  loss: 0.3018\n",
      "epoch: 120  batch:  700  loss: 0.2360\n",
      "epoch: 120  batch:  800  loss: 0.3180\n",
      "epoch: 120  batch:  900  loss: 0.1104\n",
      "epoch: 120  avg train loss: 0.2480  avg train accu: 0.930\n",
      "epoch: 120  avg test  loss: 0.1441  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 121  batch:    0  loss: 0.3650\n",
      "epoch: 121  batch:  100  loss: 0.2215\n",
      "epoch: 121  batch:  200  loss: 0.1538\n",
      "epoch: 121  batch:  300  loss: 0.1427\n",
      "epoch: 121  batch:  400  loss: 0.1301\n",
      "epoch: 121  batch:  500  loss: 0.3383\n",
      "epoch: 121  batch:  600  loss: 0.2015\n",
      "epoch: 121  batch:  700  loss: 0.3138\n",
      "epoch: 121  batch:  800  loss: 0.3511\n",
      "epoch: 121  batch:  900  loss: 0.1870\n",
      "epoch: 121  avg train loss: 0.2495  avg train accu: 0.930\n",
      "epoch: 121  avg test  loss: 0.1438  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 122  batch:    0  loss: 0.2008\n",
      "epoch: 122  batch:  100  loss: 0.2286\n",
      "epoch: 122  batch:  200  loss: 0.1900\n",
      "epoch: 122  batch:  300  loss: 0.4660\n",
      "epoch: 122  batch:  400  loss: 0.1759\n",
      "epoch: 122  batch:  500  loss: 0.1841\n",
      "epoch: 122  batch:  600  loss: 0.1850\n",
      "epoch: 122  batch:  700  loss: 0.3031\n",
      "epoch: 122  batch:  800  loss: 0.1862\n",
      "epoch: 122  batch:  900  loss: 0.2492\n",
      "epoch: 122  avg train loss: 0.2506  avg train accu: 0.929\n",
      "epoch: 122  avg test  loss: 0.1450  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 123  batch:    0  loss: 0.1895\n",
      "epoch: 123  batch:  100  loss: 0.2129\n",
      "epoch: 123  batch:  200  loss: 0.1064\n",
      "epoch: 123  batch:  300  loss: 0.2914\n",
      "epoch: 123  batch:  400  loss: 0.2326\n",
      "epoch: 123  batch:  500  loss: 0.2889\n",
      "epoch: 123  batch:  600  loss: 0.3918\n",
      "epoch: 123  batch:  700  loss: 0.1666\n",
      "epoch: 123  batch:  800  loss: 0.2965\n",
      "epoch: 123  batch:  900  loss: 0.2286\n",
      "epoch: 123  avg train loss: 0.2480  avg train accu: 0.930\n",
      "epoch: 123  avg test  loss: 0.1451  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 124  batch:    0  loss: 0.2333\n",
      "epoch: 124  batch:  100  loss: 0.1184\n",
      "epoch: 124  batch:  200  loss: 0.3208\n",
      "epoch: 124  batch:  300  loss: 0.1963\n",
      "epoch: 124  batch:  400  loss: 0.1722\n",
      "epoch: 124  batch:  500  loss: 0.1693\n",
      "epoch: 124  batch:  600  loss: 0.3830\n",
      "epoch: 124  batch:  700  loss: 0.0953\n",
      "epoch: 124  batch:  800  loss: 0.1140\n",
      "epoch: 124  batch:  900  loss: 0.2856\n",
      "epoch: 124  avg train loss: 0.2513  avg train accu: 0.930\n",
      "epoch: 124  avg test  loss: 0.1448  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 125  batch:    0  loss: 0.2183\n",
      "epoch: 125  batch:  100  loss: 0.2372\n",
      "epoch: 125  batch:  200  loss: 0.1579\n",
      "epoch: 125  batch:  300  loss: 0.3925\n",
      "epoch: 125  batch:  400  loss: 0.1198\n",
      "epoch: 125  batch:  500  loss: 0.1598\n",
      "epoch: 125  batch:  600  loss: 0.2440\n",
      "epoch: 125  batch:  700  loss: 0.3163\n",
      "epoch: 125  batch:  800  loss: 0.1940\n",
      "epoch: 125  batch:  900  loss: 0.1604\n",
      "epoch: 125  avg train loss: 0.2466  avg train accu: 0.931\n",
      "epoch: 125  avg test  loss: 0.1457  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 126  batch:    0  loss: 0.1079\n",
      "epoch: 126  batch:  100  loss: 0.2068\n",
      "epoch: 126  batch:  200  loss: 0.1746\n",
      "epoch: 126  batch:  300  loss: 0.1740\n",
      "epoch: 126  batch:  400  loss: 0.2235\n",
      "epoch: 126  batch:  500  loss: 0.2553\n",
      "epoch: 126  batch:  600  loss: 0.1925\n",
      "epoch: 126  batch:  700  loss: 0.4412\n",
      "epoch: 126  batch:  800  loss: 0.2819\n",
      "epoch: 126  batch:  900  loss: 0.2999\n",
      "epoch: 126  avg train loss: 0.2475  avg train accu: 0.930\n",
      "epoch: 126  avg test  loss: 0.1440  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 127  batch:    0  loss: 0.2603\n",
      "epoch: 127  batch:  100  loss: 0.3193\n",
      "epoch: 127  batch:  200  loss: 0.1315\n",
      "epoch: 127  batch:  300  loss: 0.2024\n",
      "epoch: 127  batch:  400  loss: 0.2641\n",
      "epoch: 127  batch:  500  loss: 0.2669\n",
      "epoch: 127  batch:  600  loss: 0.1499\n",
      "epoch: 127  batch:  700  loss: 0.1644\n",
      "epoch: 127  batch:  800  loss: 0.1471\n",
      "epoch: 127  batch:  900  loss: 0.2336\n",
      "epoch: 127  avg train loss: 0.2486  avg train accu: 0.930\n",
      "epoch: 127  avg test  loss: 0.1439  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 128  batch:    0  loss: 0.1905\n",
      "epoch: 128  batch:  100  loss: 0.2788\n",
      "epoch: 128  batch:  200  loss: 0.4990\n",
      "epoch: 128  batch:  300  loss: 0.1283\n",
      "epoch: 128  batch:  400  loss: 0.2017\n",
      "epoch: 128  batch:  500  loss: 0.2783\n",
      "epoch: 128  batch:  600  loss: 0.2943\n",
      "epoch: 128  batch:  700  loss: 0.3130\n",
      "epoch: 128  batch:  800  loss: 0.1434\n",
      "epoch: 128  batch:  900  loss: 0.2772\n",
      "epoch: 128  avg train loss: 0.2469  avg train accu: 0.930\n",
      "epoch: 128  avg test  loss: 0.1439  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 129  batch:    0  loss: 0.2327\n",
      "epoch: 129  batch:  100  loss: 0.1868\n",
      "epoch: 129  batch:  200  loss: 0.0970\n",
      "epoch: 129  batch:  300  loss: 0.2272\n",
      "epoch: 129  batch:  400  loss: 0.2068\n",
      "epoch: 129  batch:  500  loss: 0.3515\n",
      "epoch: 129  batch:  600  loss: 0.2506\n",
      "epoch: 129  batch:  700  loss: 0.2906\n",
      "epoch: 129  batch:  800  loss: 0.2381\n",
      "epoch: 129  batch:  900  loss: 0.2714\n",
      "epoch: 129  avg train loss: 0.2487  avg train accu: 0.930\n",
      "epoch: 129  avg test  loss: 0.1443  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 130  batch:    0  loss: 0.1391\n",
      "epoch: 130  batch:  100  loss: 0.3155\n",
      "epoch: 130  batch:  200  loss: 0.2263\n",
      "epoch: 130  batch:  300  loss: 0.2186\n",
      "epoch: 130  batch:  400  loss: 0.3793\n",
      "epoch: 130  batch:  500  loss: 0.4142\n",
      "epoch: 130  batch:  600  loss: 0.3485\n",
      "epoch: 130  batch:  700  loss: 0.3626\n",
      "epoch: 130  batch:  800  loss: 0.1819\n",
      "epoch: 130  batch:  900  loss: 0.2295\n",
      "epoch: 130  avg train loss: 0.2503  avg train accu: 0.930\n",
      "epoch: 130  avg test  loss: 0.1447  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 131  batch:    0  loss: 0.0791\n",
      "epoch: 131  batch:  100  loss: 0.2398\n",
      "epoch: 131  batch:  200  loss: 0.3520\n",
      "epoch: 131  batch:  300  loss: 0.2002\n",
      "epoch: 131  batch:  400  loss: 0.2121\n",
      "epoch: 131  batch:  500  loss: 0.1352\n",
      "epoch: 131  batch:  600  loss: 0.2906\n",
      "epoch: 131  batch:  700  loss: 0.3773\n",
      "epoch: 131  batch:  800  loss: 0.4063\n",
      "epoch: 131  batch:  900  loss: 0.1369\n",
      "epoch: 131  avg train loss: 0.2467  avg train accu: 0.931\n",
      "epoch: 131  avg test  loss: 0.1442  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 132  batch:    0  loss: 0.2635\n",
      "epoch: 132  batch:  100  loss: 0.2156\n",
      "epoch: 132  batch:  200  loss: 0.1897\n",
      "epoch: 132  batch:  300  loss: 0.2978\n",
      "epoch: 132  batch:  400  loss: 0.1588\n",
      "epoch: 132  batch:  500  loss: 0.1717\n",
      "epoch: 132  batch:  600  loss: 0.3446\n",
      "epoch: 132  batch:  700  loss: 0.3226\n",
      "epoch: 132  batch:  800  loss: 0.2943\n",
      "epoch: 132  batch:  900  loss: 0.1332\n",
      "epoch: 132  avg train loss: 0.2490  avg train accu: 0.931\n",
      "epoch: 132  avg test  loss: 0.1440  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 133  batch:    0  loss: 0.2185\n",
      "epoch: 133  batch:  100  loss: 0.2512\n",
      "epoch: 133  batch:  200  loss: 0.3485\n",
      "epoch: 133  batch:  300  loss: 0.2337\n",
      "epoch: 133  batch:  400  loss: 0.2948\n",
      "epoch: 133  batch:  500  loss: 0.1692\n",
      "epoch: 133  batch:  600  loss: 0.2642\n",
      "epoch: 133  batch:  700  loss: 0.2046\n",
      "epoch: 133  batch:  800  loss: 0.2600\n",
      "epoch: 133  batch:  900  loss: 0.1399\n",
      "epoch: 133  avg train loss: 0.2478  avg train accu: 0.930\n",
      "epoch: 133  avg test  loss: 0.1439  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 134  batch:    0  loss: 0.1653\n",
      "epoch: 134  batch:  100  loss: 0.1090\n",
      "epoch: 134  batch:  200  loss: 0.3877\n",
      "epoch: 134  batch:  300  loss: 0.3555\n",
      "epoch: 134  batch:  400  loss: 0.2153\n",
      "epoch: 134  batch:  500  loss: 0.2387\n",
      "epoch: 134  batch:  600  loss: 0.2570\n",
      "epoch: 134  batch:  700  loss: 0.3295\n",
      "epoch: 134  batch:  800  loss: 0.2475\n",
      "epoch: 134  batch:  900  loss: 0.2283\n",
      "epoch: 134  avg train loss: 0.2463  avg train accu: 0.930\n",
      "epoch: 134  avg test  loss: 0.1441  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 135  batch:    0  loss: 0.3283\n",
      "epoch: 135  batch:  100  loss: 0.3259\n",
      "epoch: 135  batch:  200  loss: 0.2865\n",
      "epoch: 135  batch:  300  loss: 0.2304\n",
      "epoch: 135  batch:  400  loss: 0.3274\n",
      "epoch: 135  batch:  500  loss: 0.2970\n",
      "epoch: 135  batch:  600  loss: 0.3326\n",
      "epoch: 135  batch:  700  loss: 0.2765\n",
      "epoch: 135  batch:  800  loss: 0.1818\n",
      "epoch: 135  batch:  900  loss: 0.0918\n",
      "epoch: 135  avg train loss: 0.2511  avg train accu: 0.930\n",
      "epoch: 135  avg test  loss: 0.1433  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 136  batch:    0  loss: 0.3733\n",
      "epoch: 136  batch:  100  loss: 0.3971\n",
      "epoch: 136  batch:  200  loss: 0.2728\n",
      "epoch: 136  batch:  300  loss: 0.2353\n",
      "epoch: 136  batch:  400  loss: 0.2422\n",
      "epoch: 136  batch:  500  loss: 0.3000\n",
      "epoch: 136  batch:  600  loss: 0.2915\n",
      "epoch: 136  batch:  700  loss: 0.2095\n",
      "epoch: 136  batch:  800  loss: 0.3235\n",
      "epoch: 136  batch:  900  loss: 0.2374\n",
      "epoch: 136  avg train loss: 0.2447  avg train accu: 0.931\n",
      "epoch: 136  avg test  loss: 0.1441  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 137  batch:    0  loss: 0.2844\n",
      "epoch: 137  batch:  100  loss: 0.1661\n",
      "epoch: 137  batch:  200  loss: 0.2888\n",
      "epoch: 137  batch:  300  loss: 0.3011\n",
      "epoch: 137  batch:  400  loss: 0.2125\n",
      "epoch: 137  batch:  500  loss: 0.2997\n",
      "epoch: 137  batch:  600  loss: 0.2506\n",
      "epoch: 137  batch:  700  loss: 0.3192\n",
      "epoch: 137  batch:  800  loss: 0.1485\n",
      "epoch: 137  batch:  900  loss: 0.1733\n",
      "epoch: 137  avg train loss: 0.2476  avg train accu: 0.930\n",
      "epoch: 137  avg test  loss: 0.1445  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 138  batch:    0  loss: 0.3621\n",
      "epoch: 138  batch:  100  loss: 0.2144\n",
      "epoch: 138  batch:  200  loss: 0.2055\n",
      "epoch: 138  batch:  300  loss: 0.1854\n",
      "epoch: 138  batch:  400  loss: 0.3678\n",
      "epoch: 138  batch:  500  loss: 0.1639\n",
      "epoch: 138  batch:  600  loss: 0.3731\n",
      "epoch: 138  batch:  700  loss: 0.2389\n",
      "epoch: 138  batch:  800  loss: 0.2630\n",
      "epoch: 138  batch:  900  loss: 0.3870\n",
      "epoch: 138  avg train loss: 0.2495  avg train accu: 0.931\n",
      "epoch: 138  avg test  loss: 0.1445  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 139  batch:    0  loss: 0.2391\n",
      "epoch: 139  batch:  100  loss: 0.1425\n",
      "epoch: 139  batch:  200  loss: 0.3341\n",
      "epoch: 139  batch:  300  loss: 0.2108\n",
      "epoch: 139  batch:  400  loss: 0.1977\n",
      "epoch: 139  batch:  500  loss: 0.3655\n",
      "epoch: 139  batch:  600  loss: 0.1737\n",
      "epoch: 139  batch:  700  loss: 0.2537\n",
      "epoch: 139  batch:  800  loss: 0.1201\n",
      "epoch: 139  batch:  900  loss: 0.2081\n",
      "epoch: 139  avg train loss: 0.2480  avg train accu: 0.930\n",
      "epoch: 139  avg test  loss: 0.1443  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 140  batch:    0  loss: 0.4699\n",
      "epoch: 140  batch:  100  loss: 0.3014\n",
      "epoch: 140  batch:  200  loss: 0.1480\n",
      "epoch: 140  batch:  300  loss: 0.2007\n",
      "epoch: 140  batch:  400  loss: 0.2309\n",
      "epoch: 140  batch:  500  loss: 0.1344\n",
      "epoch: 140  batch:  600  loss: 0.1666\n",
      "epoch: 140  batch:  700  loss: 0.1972\n",
      "epoch: 140  batch:  800  loss: 0.2277\n",
      "epoch: 140  batch:  900  loss: 0.1583\n",
      "epoch: 140  avg train loss: 0.2471  avg train accu: 0.929\n",
      "epoch: 140  avg test  loss: 0.1438  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 141  batch:    0  loss: 0.2376\n",
      "epoch: 141  batch:  100  loss: 0.2124\n",
      "epoch: 141  batch:  200  loss: 0.3113\n",
      "epoch: 141  batch:  300  loss: 0.3013\n",
      "epoch: 141  batch:  400  loss: 0.2372\n",
      "epoch: 141  batch:  500  loss: 0.3445\n",
      "epoch: 141  batch:  600  loss: 0.1644\n",
      "epoch: 141  batch:  700  loss: 0.1342\n",
      "epoch: 141  batch:  800  loss: 0.1621\n",
      "epoch: 141  batch:  900  loss: 0.3787\n",
      "epoch: 141  avg train loss: 0.2469  avg train accu: 0.931\n",
      "epoch: 141  avg test  loss: 0.1437  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 142  batch:    0  loss: 0.2939\n",
      "epoch: 142  batch:  100  loss: 0.2209\n",
      "epoch: 142  batch:  200  loss: 0.1919\n",
      "epoch: 142  batch:  300  loss: 0.2867\n",
      "epoch: 142  batch:  400  loss: 0.1072\n",
      "epoch: 142  batch:  500  loss: 0.1668\n",
      "epoch: 142  batch:  600  loss: 0.1865\n",
      "epoch: 142  batch:  700  loss: 0.1923\n",
      "epoch: 142  batch:  800  loss: 0.3927\n",
      "epoch: 142  batch:  900  loss: 0.1938\n",
      "epoch: 142  avg train loss: 0.2452  avg train accu: 0.931\n",
      "epoch: 142  avg test  loss: 0.1436  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 143  batch:    0  loss: 0.2692\n",
      "epoch: 143  batch:  100  loss: 0.4163\n",
      "epoch: 143  batch:  200  loss: 0.2186\n",
      "epoch: 143  batch:  300  loss: 0.2437\n",
      "epoch: 143  batch:  400  loss: 0.3125\n",
      "epoch: 143  batch:  500  loss: 0.2492\n",
      "epoch: 143  batch:  600  loss: 0.2016\n",
      "epoch: 143  batch:  700  loss: 0.3674\n",
      "epoch: 143  batch:  800  loss: 0.3921\n",
      "epoch: 143  batch:  900  loss: 0.4386\n",
      "epoch: 143  avg train loss: 0.2483  avg train accu: 0.930\n",
      "epoch: 143  avg test  loss: 0.1433  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 144  batch:    0  loss: 0.3262\n",
      "epoch: 144  batch:  100  loss: 0.1896\n",
      "epoch: 144  batch:  200  loss: 0.2419\n",
      "epoch: 144  batch:  300  loss: 0.3682\n",
      "epoch: 144  batch:  400  loss: 0.2785\n",
      "epoch: 144  batch:  500  loss: 0.2144\n",
      "epoch: 144  batch:  600  loss: 0.1827\n",
      "epoch: 144  batch:  700  loss: 0.2374\n",
      "epoch: 144  batch:  800  loss: 0.2226\n",
      "epoch: 144  batch:  900  loss: 0.1864\n",
      "epoch: 144  avg train loss: 0.2455  avg train accu: 0.931\n",
      "epoch: 144  avg test  loss: 0.1426  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 145  batch:    0  loss: 0.4475\n",
      "epoch: 145  batch:  100  loss: 0.2598\n",
      "epoch: 145  batch:  200  loss: 0.2476\n",
      "epoch: 145  batch:  300  loss: 0.1791\n",
      "epoch: 145  batch:  400  loss: 0.1475\n",
      "epoch: 145  batch:  500  loss: 0.1672\n",
      "epoch: 145  batch:  600  loss: 0.2354\n",
      "epoch: 145  batch:  700  loss: 0.1385\n",
      "epoch: 145  batch:  800  loss: 0.1981\n",
      "epoch: 145  batch:  900  loss: 0.3019\n",
      "epoch: 145  avg train loss: 0.2472  avg train accu: 0.930\n",
      "epoch: 145  avg test  loss: 0.1435  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 146  batch:    0  loss: 0.2318\n",
      "epoch: 146  batch:  100  loss: 0.3473\n",
      "epoch: 146  batch:  200  loss: 0.1610\n",
      "epoch: 146  batch:  300  loss: 0.2917\n",
      "epoch: 146  batch:  400  loss: 0.1168\n",
      "epoch: 146  batch:  500  loss: 0.2561\n",
      "epoch: 146  batch:  600  loss: 0.1887\n",
      "epoch: 146  batch:  700  loss: 0.2017\n",
      "epoch: 146  batch:  800  loss: 0.2041\n",
      "epoch: 146  batch:  900  loss: 0.2076\n",
      "epoch: 146  avg train loss: 0.2457  avg train accu: 0.931\n",
      "epoch: 146  avg test  loss: 0.1447  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 147  batch:    0  loss: 0.2240\n",
      "epoch: 147  batch:  100  loss: 0.2168\n",
      "epoch: 147  batch:  200  loss: 0.4248\n",
      "epoch: 147  batch:  300  loss: 0.3260\n",
      "epoch: 147  batch:  400  loss: 0.3443\n",
      "epoch: 147  batch:  500  loss: 0.2295\n",
      "epoch: 147  batch:  600  loss: 0.4385\n",
      "epoch: 147  batch:  700  loss: 0.3516\n",
      "epoch: 147  batch:  800  loss: 0.3791\n",
      "epoch: 147  batch:  900  loss: 0.2003\n",
      "epoch: 147  avg train loss: 0.2454  avg train accu: 0.931\n",
      "epoch: 147  avg test  loss: 0.1432  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 148  batch:    0  loss: 0.3169\n",
      "epoch: 148  batch:  100  loss: 0.2910\n",
      "epoch: 148  batch:  200  loss: 0.2304\n",
      "epoch: 148  batch:  300  loss: 0.1490\n",
      "epoch: 148  batch:  400  loss: 0.2618\n",
      "epoch: 148  batch:  500  loss: 0.1116\n",
      "epoch: 148  batch:  600  loss: 0.2278\n",
      "epoch: 148  batch:  700  loss: 0.1141\n",
      "epoch: 148  batch:  800  loss: 0.3066\n",
      "epoch: 148  batch:  900  loss: 0.2201\n",
      "epoch: 148  avg train loss: 0.2452  avg train accu: 0.932\n",
      "epoch: 148  avg test  loss: 0.1435  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 149  batch:    0  loss: 0.2883\n",
      "epoch: 149  batch:  100  loss: 0.1837\n",
      "epoch: 149  batch:  200  loss: 0.2461\n",
      "epoch: 149  batch:  300  loss: 0.1290\n",
      "epoch: 149  batch:  400  loss: 0.2051\n",
      "epoch: 149  batch:  500  loss: 0.1673\n",
      "epoch: 149  batch:  600  loss: 0.3140\n",
      "epoch: 149  batch:  700  loss: 0.2080\n",
      "epoch: 149  batch:  800  loss: 0.2908\n",
      "epoch: 149  batch:  900  loss: 0.2751\n",
      "epoch: 149  avg train loss: 0.2463  avg train accu: 0.931\n",
      "epoch: 149  avg test  loss: 0.1434  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 150  batch:    0  loss: 0.2837\n",
      "epoch: 150  batch:  100  loss: 0.2391\n",
      "epoch: 150  batch:  200  loss: 0.2038\n",
      "epoch: 150  batch:  300  loss: 0.1719\n",
      "epoch: 150  batch:  400  loss: 0.2960\n",
      "epoch: 150  batch:  500  loss: 0.1523\n",
      "epoch: 150  batch:  600  loss: 0.1933\n",
      "epoch: 150  batch:  700  loss: 0.2940\n",
      "epoch: 150  batch:  800  loss: 0.3216\n",
      "epoch: 150  batch:  900  loss: 0.2847\n",
      "epoch: 150  avg train loss: 0.2471  avg train accu: 0.930\n",
      "epoch: 150  avg test  loss: 0.1441  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 151  batch:    0  loss: 0.3184\n",
      "epoch: 151  batch:  100  loss: 0.2199\n",
      "epoch: 151  batch:  200  loss: 0.2171\n",
      "epoch: 151  batch:  300  loss: 0.2436\n",
      "epoch: 151  batch:  400  loss: 0.1534\n",
      "epoch: 151  batch:  500  loss: 0.1832\n",
      "epoch: 151  batch:  600  loss: 0.2369\n",
      "epoch: 151  batch:  700  loss: 0.1270\n",
      "epoch: 151  batch:  800  loss: 0.2435\n",
      "epoch: 151  batch:  900  loss: 0.2037\n",
      "epoch: 151  avg train loss: 0.2473  avg train accu: 0.929\n",
      "epoch: 151  avg test  loss: 0.1428  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 152  batch:    0  loss: 0.1244\n",
      "epoch: 152  batch:  100  loss: 0.2267\n",
      "epoch: 152  batch:  200  loss: 0.2080\n",
      "epoch: 152  batch:  300  loss: 0.1568\n",
      "epoch: 152  batch:  400  loss: 0.3249\n",
      "epoch: 152  batch:  500  loss: 0.1361\n",
      "epoch: 152  batch:  600  loss: 0.3393\n",
      "epoch: 152  batch:  700  loss: 0.1677\n",
      "epoch: 152  batch:  800  loss: 0.2727\n",
      "epoch: 152  batch:  900  loss: 0.2710\n",
      "epoch: 152  avg train loss: 0.2435  avg train accu: 0.932\n",
      "epoch: 152  avg test  loss: 0.1435  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 153  batch:    0  loss: 0.1907\n",
      "epoch: 153  batch:  100  loss: 0.1557\n",
      "epoch: 153  batch:  200  loss: 0.2013\n",
      "epoch: 153  batch:  300  loss: 0.2750\n",
      "epoch: 153  batch:  400  loss: 0.2027\n",
      "epoch: 153  batch:  500  loss: 0.2487\n",
      "epoch: 153  batch:  600  loss: 0.1705\n",
      "epoch: 153  batch:  700  loss: 0.4131\n",
      "epoch: 153  batch:  800  loss: 0.4692\n",
      "epoch: 153  batch:  900  loss: 0.1250\n",
      "epoch: 153  avg train loss: 0.2440  avg train accu: 0.931\n",
      "epoch: 153  avg test  loss: 0.1425  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 154  batch:    0  loss: 0.2436\n",
      "epoch: 154  batch:  100  loss: 0.1560\n",
      "epoch: 154  batch:  200  loss: 0.1640\n",
      "epoch: 154  batch:  300  loss: 0.1606\n",
      "epoch: 154  batch:  400  loss: 0.3872\n",
      "epoch: 154  batch:  500  loss: 0.2233\n",
      "epoch: 154  batch:  600  loss: 0.1479\n",
      "epoch: 154  batch:  700  loss: 0.1950\n",
      "epoch: 154  batch:  800  loss: 0.3017\n",
      "epoch: 154  batch:  900  loss: 0.2538\n",
      "epoch: 154  avg train loss: 0.2457  avg train accu: 0.931\n",
      "epoch: 154  avg test  loss: 0.1426  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 155  batch:    0  loss: 0.2302\n",
      "epoch: 155  batch:  100  loss: 0.4411\n",
      "epoch: 155  batch:  200  loss: 0.2877\n",
      "epoch: 155  batch:  300  loss: 0.2198\n",
      "epoch: 155  batch:  400  loss: 0.1901\n",
      "epoch: 155  batch:  500  loss: 0.2260\n",
      "epoch: 155  batch:  600  loss: 0.2240\n",
      "epoch: 155  batch:  700  loss: 0.1347\n",
      "epoch: 155  batch:  800  loss: 0.1918\n",
      "epoch: 155  batch:  900  loss: 0.2473\n",
      "epoch: 155  avg train loss: 0.2473  avg train accu: 0.930\n",
      "epoch: 155  avg test  loss: 0.1433  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 156  batch:    0  loss: 0.2304\n",
      "epoch: 156  batch:  100  loss: 0.2827\n",
      "epoch: 156  batch:  200  loss: 0.2412\n",
      "epoch: 156  batch:  300  loss: 0.4073\n",
      "epoch: 156  batch:  400  loss: 0.2848\n",
      "epoch: 156  batch:  500  loss: 0.3816\n",
      "epoch: 156  batch:  600  loss: 0.3354\n",
      "epoch: 156  batch:  700  loss: 0.1745\n",
      "epoch: 156  batch:  800  loss: 0.1988\n",
      "epoch: 156  batch:  900  loss: 0.2289\n",
      "epoch: 156  avg train loss: 0.2468  avg train accu: 0.930\n",
      "epoch: 156  avg test  loss: 0.1424  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 157  batch:    0  loss: 0.2431\n",
      "epoch: 157  batch:  100  loss: 0.1291\n",
      "epoch: 157  batch:  200  loss: 0.1439\n",
      "epoch: 157  batch:  300  loss: 0.1453\n",
      "epoch: 157  batch:  400  loss: 0.1296\n",
      "epoch: 157  batch:  500  loss: 0.1386\n",
      "epoch: 157  batch:  600  loss: 0.3166\n",
      "epoch: 157  batch:  700  loss: 0.3840\n",
      "epoch: 157  batch:  800  loss: 0.3144\n",
      "epoch: 157  batch:  900  loss: 0.3275\n",
      "epoch: 157  avg train loss: 0.2477  avg train accu: 0.931\n",
      "epoch: 157  avg test  loss: 0.1431  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 158  batch:    0  loss: 0.3191\n",
      "epoch: 158  batch:  100  loss: 0.2750\n",
      "epoch: 158  batch:  200  loss: 0.1987\n",
      "epoch: 158  batch:  300  loss: 0.1634\n",
      "epoch: 158  batch:  400  loss: 0.1936\n",
      "epoch: 158  batch:  500  loss: 0.3958\n",
      "epoch: 158  batch:  600  loss: 0.2150\n",
      "epoch: 158  batch:  700  loss: 0.2596\n",
      "epoch: 158  batch:  800  loss: 0.2142\n",
      "epoch: 158  batch:  900  loss: 0.2663\n",
      "epoch: 158  avg train loss: 0.2468  avg train accu: 0.930\n",
      "epoch: 158  avg test  loss: 0.1427  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 159  batch:    0  loss: 0.1972\n",
      "epoch: 159  batch:  100  loss: 0.1824\n",
      "epoch: 159  batch:  200  loss: 0.1657\n",
      "epoch: 159  batch:  300  loss: 0.2153\n",
      "epoch: 159  batch:  400  loss: 0.2960\n",
      "epoch: 159  batch:  500  loss: 0.2352\n",
      "epoch: 159  batch:  600  loss: 0.2785\n",
      "epoch: 159  batch:  700  loss: 0.1038\n",
      "epoch: 159  batch:  800  loss: 0.3304\n",
      "epoch: 159  batch:  900  loss: 0.1707\n",
      "epoch: 159  avg train loss: 0.2458  avg train accu: 0.932\n",
      "epoch: 159  avg test  loss: 0.1428  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 160  batch:    0  loss: 0.1932\n",
      "epoch: 160  batch:  100  loss: 0.1733\n",
      "epoch: 160  batch:  200  loss: 0.3578\n",
      "epoch: 160  batch:  300  loss: 0.1853\n",
      "epoch: 160  batch:  400  loss: 0.2893\n",
      "epoch: 160  batch:  500  loss: 0.1968\n",
      "epoch: 160  batch:  600  loss: 0.2995\n",
      "epoch: 160  batch:  700  loss: 0.1603\n",
      "epoch: 160  batch:  800  loss: 0.1812\n",
      "epoch: 160  batch:  900  loss: 0.2419\n",
      "epoch: 160  avg train loss: 0.2474  avg train accu: 0.930\n",
      "epoch: 160  avg test  loss: 0.1429  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 161  batch:    0  loss: 0.1432\n",
      "epoch: 161  batch:  100  loss: 0.2141\n",
      "epoch: 161  batch:  200  loss: 0.1310\n",
      "epoch: 161  batch:  300  loss: 0.1374\n",
      "epoch: 161  batch:  400  loss: 0.2944\n",
      "epoch: 161  batch:  500  loss: 0.3248\n",
      "epoch: 161  batch:  600  loss: 0.1168\n",
      "epoch: 161  batch:  700  loss: 0.1756\n",
      "epoch: 161  batch:  800  loss: 0.2142\n",
      "epoch: 161  batch:  900  loss: 0.1895\n",
      "epoch: 161  avg train loss: 0.2467  avg train accu: 0.930\n",
      "epoch: 161  avg test  loss: 0.1429  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 162  batch:    0  loss: 0.2933\n",
      "epoch: 162  batch:  100  loss: 0.3967\n",
      "epoch: 162  batch:  200  loss: 0.1227\n",
      "epoch: 162  batch:  300  loss: 0.2542\n",
      "epoch: 162  batch:  400  loss: 0.1790\n",
      "epoch: 162  batch:  500  loss: 0.2258\n",
      "epoch: 162  batch:  600  loss: 0.1726\n",
      "epoch: 162  batch:  700  loss: 0.2597\n",
      "epoch: 162  batch:  800  loss: 0.2821\n",
      "epoch: 162  batch:  900  loss: 0.2213\n",
      "epoch: 162  avg train loss: 0.2463  avg train accu: 0.931\n",
      "epoch: 162  avg test  loss: 0.1426  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 163  batch:    0  loss: 0.2278\n",
      "epoch: 163  batch:  100  loss: 0.2713\n",
      "epoch: 163  batch:  200  loss: 0.2440\n",
      "epoch: 163  batch:  300  loss: 0.1726\n",
      "epoch: 163  batch:  400  loss: 0.2473\n",
      "epoch: 163  batch:  500  loss: 0.1437\n",
      "epoch: 163  batch:  600  loss: 0.1877\n",
      "epoch: 163  batch:  700  loss: 0.1963\n",
      "epoch: 163  batch:  800  loss: 0.2948\n",
      "epoch: 163  batch:  900  loss: 0.2731\n",
      "epoch: 163  avg train loss: 0.2470  avg train accu: 0.931\n",
      "epoch: 163  avg test  loss: 0.1422  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 164  batch:    0  loss: 0.2388\n",
      "epoch: 164  batch:  100  loss: 0.2431\n",
      "epoch: 164  batch:  200  loss: 0.2543\n",
      "epoch: 164  batch:  300  loss: 0.2812\n",
      "epoch: 164  batch:  400  loss: 0.3368\n",
      "epoch: 164  batch:  500  loss: 0.2610\n",
      "epoch: 164  batch:  600  loss: 0.1109\n",
      "epoch: 164  batch:  700  loss: 0.3163\n",
      "epoch: 164  batch:  800  loss: 0.4429\n",
      "epoch: 164  batch:  900  loss: 0.1795\n",
      "epoch: 164  avg train loss: 0.2487  avg train accu: 0.930\n",
      "epoch: 164  avg test  loss: 0.1430  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 165  batch:    0  loss: 0.2594\n",
      "epoch: 165  batch:  100  loss: 0.4316\n",
      "epoch: 165  batch:  200  loss: 0.1904\n",
      "epoch: 165  batch:  300  loss: 0.3413\n",
      "epoch: 165  batch:  400  loss: 0.1308\n",
      "epoch: 165  batch:  500  loss: 0.5192\n",
      "epoch: 165  batch:  600  loss: 0.2109\n",
      "epoch: 165  batch:  700  loss: 0.2932\n",
      "epoch: 165  batch:  800  loss: 0.1873\n",
      "epoch: 165  batch:  900  loss: 0.2074\n",
      "epoch: 165  avg train loss: 0.2452  avg train accu: 0.930\n",
      "epoch: 165  avg test  loss: 0.1423  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 166  batch:    0  loss: 0.2202\n",
      "epoch: 166  batch:  100  loss: 0.2747\n",
      "epoch: 166  batch:  200  loss: 0.1703\n",
      "epoch: 166  batch:  300  loss: 0.3042\n",
      "epoch: 166  batch:  400  loss: 0.1966\n",
      "epoch: 166  batch:  500  loss: 0.2596\n",
      "epoch: 166  batch:  600  loss: 0.3122\n",
      "epoch: 166  batch:  700  loss: 0.2918\n",
      "epoch: 166  batch:  800  loss: 0.3057\n",
      "epoch: 166  batch:  900  loss: 0.1775\n",
      "epoch: 166  avg train loss: 0.2452  avg train accu: 0.931\n",
      "epoch: 166  avg test  loss: 0.1418  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 167  batch:    0  loss: 0.2675\n",
      "epoch: 167  batch:  100  loss: 0.1541\n",
      "epoch: 167  batch:  200  loss: 0.2622\n",
      "epoch: 167  batch:  300  loss: 0.3027\n",
      "epoch: 167  batch:  400  loss: 0.1978\n",
      "epoch: 167  batch:  500  loss: 0.2517\n",
      "epoch: 167  batch:  600  loss: 0.1183\n",
      "epoch: 167  batch:  700  loss: 0.2310\n",
      "epoch: 167  batch:  800  loss: 0.1854\n",
      "epoch: 167  batch:  900  loss: 0.1677\n",
      "epoch: 167  avg train loss: 0.2437  avg train accu: 0.932\n",
      "epoch: 167  avg test  loss: 0.1420  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 168  batch:    0  loss: 0.1472\n",
      "epoch: 168  batch:  100  loss: 0.2068\n",
      "epoch: 168  batch:  200  loss: 0.1777\n",
      "epoch: 168  batch:  300  loss: 0.2996\n",
      "epoch: 168  batch:  400  loss: 0.2942\n",
      "epoch: 168  batch:  500  loss: 0.1723\n",
      "epoch: 168  batch:  600  loss: 0.1339\n",
      "epoch: 168  batch:  700  loss: 0.3928\n",
      "epoch: 168  batch:  800  loss: 0.2138\n",
      "epoch: 168  batch:  900  loss: 0.4241\n",
      "epoch: 168  avg train loss: 0.2454  avg train accu: 0.931\n",
      "epoch: 168  avg test  loss: 0.1422  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 169  batch:    0  loss: 0.2261\n",
      "epoch: 169  batch:  100  loss: 0.1948\n",
      "epoch: 169  batch:  200  loss: 0.2612\n",
      "epoch: 169  batch:  300  loss: 0.3209\n",
      "epoch: 169  batch:  400  loss: 0.2617\n",
      "epoch: 169  batch:  500  loss: 0.2698\n",
      "epoch: 169  batch:  600  loss: 0.2573\n",
      "epoch: 169  batch:  700  loss: 0.3283\n",
      "epoch: 169  batch:  800  loss: 0.3704\n",
      "epoch: 169  batch:  900  loss: 0.3966\n",
      "epoch: 169  avg train loss: 0.2469  avg train accu: 0.930\n",
      "epoch: 169  avg test  loss: 0.1429  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 170  batch:    0  loss: 0.1521\n",
      "epoch: 170  batch:  100  loss: 0.3811\n",
      "epoch: 170  batch:  200  loss: 0.2823\n",
      "epoch: 170  batch:  300  loss: 0.2537\n",
      "epoch: 170  batch:  400  loss: 0.1159\n",
      "epoch: 170  batch:  500  loss: 0.2432\n",
      "epoch: 170  batch:  600  loss: 0.2811\n",
      "epoch: 170  batch:  700  loss: 0.2369\n",
      "epoch: 170  batch:  800  loss: 0.1214\n",
      "epoch: 170  batch:  900  loss: 0.2069\n",
      "epoch: 170  avg train loss: 0.2467  avg train accu: 0.931\n",
      "epoch: 170  avg test  loss: 0.1417  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 171  batch:    0  loss: 0.1807\n",
      "epoch: 171  batch:  100  loss: 0.1169\n",
      "epoch: 171  batch:  200  loss: 0.2851\n",
      "epoch: 171  batch:  300  loss: 0.2165\n",
      "epoch: 171  batch:  400  loss: 0.2051\n",
      "epoch: 171  batch:  500  loss: 0.2533\n",
      "epoch: 171  batch:  600  loss: 0.1826\n",
      "epoch: 171  batch:  700  loss: 0.1211\n",
      "epoch: 171  batch:  800  loss: 0.3507\n",
      "epoch: 171  batch:  900  loss: 0.3068\n",
      "epoch: 171  avg train loss: 0.2420  avg train accu: 0.932\n",
      "epoch: 171  avg test  loss: 0.1425  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 172  batch:    0  loss: 0.2572\n",
      "epoch: 172  batch:  100  loss: 0.1726\n",
      "epoch: 172  batch:  200  loss: 0.2324\n",
      "epoch: 172  batch:  300  loss: 0.2546\n",
      "epoch: 172  batch:  400  loss: 0.2174\n",
      "epoch: 172  batch:  500  loss: 0.3613\n",
      "epoch: 172  batch:  600  loss: 0.2661\n",
      "epoch: 172  batch:  700  loss: 0.2014\n",
      "epoch: 172  batch:  800  loss: 0.3269\n",
      "epoch: 172  batch:  900  loss: 0.2948\n",
      "epoch: 172  avg train loss: 0.2473  avg train accu: 0.930\n",
      "epoch: 172  avg test  loss: 0.1424  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 173  batch:    0  loss: 0.1363\n",
      "epoch: 173  batch:  100  loss: 0.1632\n",
      "epoch: 173  batch:  200  loss: 0.2872\n",
      "epoch: 173  batch:  300  loss: 0.4095\n",
      "epoch: 173  batch:  400  loss: 0.2478\n",
      "epoch: 173  batch:  500  loss: 0.2251\n",
      "epoch: 173  batch:  600  loss: 0.4833\n",
      "epoch: 173  batch:  700  loss: 0.4644\n",
      "epoch: 173  batch:  800  loss: 0.1493\n",
      "epoch: 173  batch:  900  loss: 0.2949\n",
      "epoch: 173  avg train loss: 0.2451  avg train accu: 0.931\n",
      "epoch: 173  avg test  loss: 0.1423  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 174  batch:    0  loss: 0.3449\n",
      "epoch: 174  batch:  100  loss: 0.1412\n",
      "epoch: 174  batch:  200  loss: 0.2424\n",
      "epoch: 174  batch:  300  loss: 0.1583\n",
      "epoch: 174  batch:  400  loss: 0.3411\n",
      "epoch: 174  batch:  500  loss: 0.2673\n",
      "epoch: 174  batch:  600  loss: 0.1772\n",
      "epoch: 174  batch:  700  loss: 0.2284\n",
      "epoch: 174  batch:  800  loss: 0.2493\n",
      "epoch: 174  batch:  900  loss: 0.4151\n",
      "epoch: 174  avg train loss: 0.2463  avg train accu: 0.930\n",
      "epoch: 174  avg test  loss: 0.1418  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 175  batch:    0  loss: 0.2466\n",
      "epoch: 175  batch:  100  loss: 0.3643\n",
      "epoch: 175  batch:  200  loss: 0.2866\n",
      "epoch: 175  batch:  300  loss: 0.4322\n",
      "epoch: 175  batch:  400  loss: 0.2005\n",
      "epoch: 175  batch:  500  loss: 0.3627\n",
      "epoch: 175  batch:  600  loss: 0.1563\n",
      "epoch: 175  batch:  700  loss: 0.1684\n",
      "epoch: 175  batch:  800  loss: 0.2805\n",
      "epoch: 175  batch:  900  loss: 0.2560\n",
      "epoch: 175  avg train loss: 0.2462  avg train accu: 0.931\n",
      "epoch: 175  avg test  loss: 0.1432  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 176  batch:    0  loss: 0.2065\n",
      "epoch: 176  batch:  100  loss: 0.1242\n",
      "epoch: 176  batch:  200  loss: 0.2405\n",
      "epoch: 176  batch:  300  loss: 0.1702\n",
      "epoch: 176  batch:  400  loss: 0.3102\n",
      "epoch: 176  batch:  500  loss: 0.1883\n",
      "epoch: 176  batch:  600  loss: 0.2659\n",
      "epoch: 176  batch:  700  loss: 0.1751\n",
      "epoch: 176  batch:  800  loss: 0.3040\n",
      "epoch: 176  batch:  900  loss: 0.2025\n",
      "epoch: 176  avg train loss: 0.2468  avg train accu: 0.930\n",
      "epoch: 176  avg test  loss: 0.1428  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 177  batch:    0  loss: 0.1758\n",
      "epoch: 177  batch:  100  loss: 0.2193\n",
      "epoch: 177  batch:  200  loss: 0.3613\n",
      "epoch: 177  batch:  300  loss: 0.3773\n",
      "epoch: 177  batch:  400  loss: 0.1614\n",
      "epoch: 177  batch:  500  loss: 0.1912\n",
      "epoch: 177  batch:  600  loss: 0.2961\n",
      "epoch: 177  batch:  700  loss: 0.3830\n",
      "epoch: 177  batch:  800  loss: 0.2402\n",
      "epoch: 177  batch:  900  loss: 0.2337\n",
      "epoch: 177  avg train loss: 0.2483  avg train accu: 0.929\n",
      "epoch: 177  avg test  loss: 0.1423  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 178  batch:    0  loss: 0.1126\n",
      "epoch: 178  batch:  100  loss: 0.0805\n",
      "epoch: 178  batch:  200  loss: 0.1603\n",
      "epoch: 178  batch:  300  loss: 0.1549\n",
      "epoch: 178  batch:  400  loss: 0.3436\n",
      "epoch: 178  batch:  500  loss: 0.2911\n",
      "epoch: 178  batch:  600  loss: 0.2240\n",
      "epoch: 178  batch:  700  loss: 0.1578\n",
      "epoch: 178  batch:  800  loss: 0.2387\n",
      "epoch: 178  batch:  900  loss: 0.1874\n",
      "epoch: 178  avg train loss: 0.2462  avg train accu: 0.931\n",
      "epoch: 178  avg test  loss: 0.1423  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 179  batch:    0  loss: 0.3532\n",
      "epoch: 179  batch:  100  loss: 0.1964\n",
      "epoch: 179  batch:  200  loss: 0.1740\n",
      "epoch: 179  batch:  300  loss: 0.2135\n",
      "epoch: 179  batch:  400  loss: 0.1731\n",
      "epoch: 179  batch:  500  loss: 0.2303\n",
      "epoch: 179  batch:  600  loss: 0.2381\n",
      "epoch: 179  batch:  700  loss: 0.1793\n",
      "epoch: 179  batch:  800  loss: 0.3647\n",
      "epoch: 179  batch:  900  loss: 0.2001\n",
      "epoch: 179  avg train loss: 0.2452  avg train accu: 0.931\n",
      "epoch: 179  avg test  loss: 0.1415  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 180  batch:    0  loss: 0.2041\n",
      "epoch: 180  batch:  100  loss: 0.3180\n",
      "epoch: 180  batch:  200  loss: 0.3154\n",
      "epoch: 180  batch:  300  loss: 0.1760\n",
      "epoch: 180  batch:  400  loss: 0.2745\n",
      "epoch: 180  batch:  500  loss: 0.3552\n",
      "epoch: 180  batch:  600  loss: 0.1070\n",
      "epoch: 180  batch:  700  loss: 0.1778\n",
      "epoch: 180  batch:  800  loss: 0.3500\n",
      "epoch: 180  batch:  900  loss: 0.1727\n",
      "epoch: 180  avg train loss: 0.2434  avg train accu: 0.932\n",
      "epoch: 180  avg test  loss: 0.1420  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 181  batch:    0  loss: 0.2026\n",
      "epoch: 181  batch:  100  loss: 0.3789\n",
      "epoch: 181  batch:  200  loss: 0.3935\n",
      "epoch: 181  batch:  300  loss: 0.2348\n",
      "epoch: 181  batch:  400  loss: 0.2146\n",
      "epoch: 181  batch:  500  loss: 0.3435\n",
      "epoch: 181  batch:  600  loss: 0.2653\n",
      "epoch: 181  batch:  700  loss: 0.2775\n",
      "epoch: 181  batch:  800  loss: 0.1768\n",
      "epoch: 181  batch:  900  loss: 0.2404\n",
      "epoch: 181  avg train loss: 0.2451  avg train accu: 0.932\n",
      "epoch: 181  avg test  loss: 0.1414  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 182  batch:    0  loss: 0.2347\n",
      "epoch: 182  batch:  100  loss: 0.2065\n",
      "epoch: 182  batch:  200  loss: 0.3742\n",
      "epoch: 182  batch:  300  loss: 0.2560\n",
      "epoch: 182  batch:  400  loss: 0.3600\n",
      "epoch: 182  batch:  500  loss: 0.3996\n",
      "epoch: 182  batch:  600  loss: 0.2100\n",
      "epoch: 182  batch:  700  loss: 0.2451\n",
      "epoch: 182  batch:  800  loss: 0.4213\n",
      "epoch: 182  batch:  900  loss: 0.1604\n",
      "epoch: 182  avg train loss: 0.2481  avg train accu: 0.930\n",
      "epoch: 182  avg test  loss: 0.1418  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 183  batch:    0  loss: 0.1757\n",
      "epoch: 183  batch:  100  loss: 0.2264\n",
      "epoch: 183  batch:  200  loss: 0.2509\n",
      "epoch: 183  batch:  300  loss: 0.1985\n",
      "epoch: 183  batch:  400  loss: 0.1346\n",
      "epoch: 183  batch:  500  loss: 0.1614\n",
      "epoch: 183  batch:  600  loss: 0.2938\n",
      "epoch: 183  batch:  700  loss: 0.2401\n",
      "epoch: 183  batch:  800  loss: 0.4799\n",
      "epoch: 183  batch:  900  loss: 0.1425\n",
      "epoch: 183  avg train loss: 0.2463  avg train accu: 0.931\n",
      "epoch: 183  avg test  loss: 0.1419  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 184  batch:    0  loss: 0.1448\n",
      "epoch: 184  batch:  100  loss: 0.2511\n",
      "epoch: 184  batch:  200  loss: 0.2031\n",
      "epoch: 184  batch:  300  loss: 0.3735\n",
      "epoch: 184  batch:  400  loss: 0.1291\n",
      "epoch: 184  batch:  500  loss: 0.1750\n",
      "epoch: 184  batch:  600  loss: 0.2710\n",
      "epoch: 184  batch:  700  loss: 0.3355\n",
      "epoch: 184  batch:  800  loss: 0.3254\n",
      "epoch: 184  batch:  900  loss: 0.1969\n",
      "epoch: 184  avg train loss: 0.2448  avg train accu: 0.931\n",
      "epoch: 184  avg test  loss: 0.1409  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 185  batch:    0  loss: 0.3383\n",
      "epoch: 185  batch:  100  loss: 0.1166\n",
      "epoch: 185  batch:  200  loss: 0.2509\n",
      "epoch: 185  batch:  300  loss: 0.1807\n",
      "epoch: 185  batch:  400  loss: 0.3670\n",
      "epoch: 185  batch:  500  loss: 0.3136\n",
      "epoch: 185  batch:  600  loss: 0.3268\n",
      "epoch: 185  batch:  700  loss: 0.2377\n",
      "epoch: 185  batch:  800  loss: 0.3639\n",
      "epoch: 185  batch:  900  loss: 0.1706\n",
      "epoch: 185  avg train loss: 0.2451  avg train accu: 0.932\n",
      "epoch: 185  avg test  loss: 0.1416  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 186  batch:    0  loss: 0.1709\n",
      "epoch: 186  batch:  100  loss: 0.3214\n",
      "epoch: 186  batch:  200  loss: 0.1449\n",
      "epoch: 186  batch:  300  loss: 0.2724\n",
      "epoch: 186  batch:  400  loss: 0.2867\n",
      "epoch: 186  batch:  500  loss: 0.2864\n",
      "epoch: 186  batch:  600  loss: 0.2080\n",
      "epoch: 186  batch:  700  loss: 0.1765\n",
      "epoch: 186  batch:  800  loss: 0.2567\n",
      "epoch: 186  batch:  900  loss: 0.1929\n",
      "epoch: 186  avg train loss: 0.2433  avg train accu: 0.933\n",
      "epoch: 186  avg test  loss: 0.1419  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 187  batch:    0  loss: 0.3389\n",
      "epoch: 187  batch:  100  loss: 0.1942\n",
      "epoch: 187  batch:  200  loss: 0.1940\n",
      "epoch: 187  batch:  300  loss: 0.2541\n",
      "epoch: 187  batch:  400  loss: 0.3296\n",
      "epoch: 187  batch:  500  loss: 0.3799\n",
      "epoch: 187  batch:  600  loss: 0.1361\n",
      "epoch: 187  batch:  700  loss: 0.1628\n",
      "epoch: 187  batch:  800  loss: 0.1830\n",
      "epoch: 187  batch:  900  loss: 0.3259\n",
      "epoch: 187  avg train loss: 0.2453  avg train accu: 0.931\n",
      "epoch: 187  avg test  loss: 0.1422  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 188  batch:    0  loss: 0.2606\n",
      "epoch: 188  batch:  100  loss: 0.1972\n",
      "epoch: 188  batch:  200  loss: 0.1386\n",
      "epoch: 188  batch:  300  loss: 0.2721\n",
      "epoch: 188  batch:  400  loss: 0.1871\n",
      "epoch: 188  batch:  500  loss: 0.2201\n",
      "epoch: 188  batch:  600  loss: 0.2170\n",
      "epoch: 188  batch:  700  loss: 0.2397\n",
      "epoch: 188  batch:  800  loss: 0.2370\n",
      "epoch: 188  batch:  900  loss: 0.3113\n",
      "epoch: 188  avg train loss: 0.2454  avg train accu: 0.931\n",
      "epoch: 188  avg test  loss: 0.1418  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 189  batch:    0  loss: 0.2562\n",
      "epoch: 189  batch:  100  loss: 0.1288\n",
      "epoch: 189  batch:  200  loss: 0.2235\n",
      "epoch: 189  batch:  300  loss: 0.1894\n",
      "epoch: 189  batch:  400  loss: 0.1354\n",
      "epoch: 189  batch:  500  loss: 0.2143\n",
      "epoch: 189  batch:  600  loss: 0.1681\n",
      "epoch: 189  batch:  700  loss: 0.2841\n",
      "epoch: 189  batch:  800  loss: 0.2643\n",
      "epoch: 189  batch:  900  loss: 0.4085\n",
      "epoch: 189  avg train loss: 0.2442  avg train accu: 0.931\n",
      "epoch: 189  avg test  loss: 0.1410  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 190  batch:    0  loss: 0.1612\n",
      "epoch: 190  batch:  100  loss: 0.4486\n",
      "epoch: 190  batch:  200  loss: 0.2061\n",
      "epoch: 190  batch:  300  loss: 0.2858\n",
      "epoch: 190  batch:  400  loss: 0.2564\n",
      "epoch: 190  batch:  500  loss: 0.3445\n",
      "epoch: 190  batch:  600  loss: 0.1850\n",
      "epoch: 190  batch:  700  loss: 0.1941\n",
      "epoch: 190  batch:  800  loss: 0.2071\n",
      "epoch: 190  batch:  900  loss: 0.2747\n",
      "epoch: 190  avg train loss: 0.2411  avg train accu: 0.932\n",
      "epoch: 190  avg test  loss: 0.1419  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 191  batch:    0  loss: 0.1898\n",
      "epoch: 191  batch:  100  loss: 0.3727\n",
      "epoch: 191  batch:  200  loss: 0.1969\n",
      "epoch: 191  batch:  300  loss: 0.2603\n",
      "epoch: 191  batch:  400  loss: 0.3016\n",
      "epoch: 191  batch:  500  loss: 0.2633\n",
      "epoch: 191  batch:  600  loss: 0.1788\n",
      "epoch: 191  batch:  700  loss: 0.0876\n",
      "epoch: 191  batch:  800  loss: 0.1468\n",
      "epoch: 191  batch:  900  loss: 0.1952\n",
      "epoch: 191  avg train loss: 0.2449  avg train accu: 0.932\n",
      "epoch: 191  avg test  loss: 0.1425  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 192  batch:    0  loss: 0.3728\n",
      "epoch: 192  batch:  100  loss: 0.1935\n",
      "epoch: 192  batch:  200  loss: 0.2484\n",
      "epoch: 192  batch:  300  loss: 0.4137\n",
      "epoch: 192  batch:  400  loss: 0.2400\n",
      "epoch: 192  batch:  500  loss: 0.2799\n",
      "epoch: 192  batch:  600  loss: 0.2327\n",
      "epoch: 192  batch:  700  loss: 0.1805\n",
      "epoch: 192  batch:  800  loss: 0.3131\n",
      "epoch: 192  batch:  900  loss: 0.1787\n",
      "epoch: 192  avg train loss: 0.2432  avg train accu: 0.932\n",
      "epoch: 192  avg test  loss: 0.1425  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 193  batch:    0  loss: 0.2479\n",
      "epoch: 193  batch:  100  loss: 0.3015\n",
      "epoch: 193  batch:  200  loss: 0.2639\n",
      "epoch: 193  batch:  300  loss: 0.2529\n",
      "epoch: 193  batch:  400  loss: 0.4373\n",
      "epoch: 193  batch:  500  loss: 0.3455\n",
      "epoch: 193  batch:  600  loss: 0.2794\n",
      "epoch: 193  batch:  700  loss: 0.3036\n",
      "epoch: 193  batch:  800  loss: 0.1606\n",
      "epoch: 193  batch:  900  loss: 0.2470\n",
      "epoch: 193  avg train loss: 0.2448  avg train accu: 0.932\n",
      "epoch: 193  avg test  loss: 0.1424  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 194  batch:    0  loss: 0.2479\n",
      "epoch: 194  batch:  100  loss: 0.2490\n",
      "epoch: 194  batch:  200  loss: 0.3657\n",
      "epoch: 194  batch:  300  loss: 0.4100\n",
      "epoch: 194  batch:  400  loss: 0.1771\n",
      "epoch: 194  batch:  500  loss: 0.4370\n",
      "epoch: 194  batch:  600  loss: 0.2515\n",
      "epoch: 194  batch:  700  loss: 0.2027\n",
      "epoch: 194  batch:  800  loss: 0.1269\n",
      "epoch: 194  batch:  900  loss: 0.2805\n",
      "epoch: 194  avg train loss: 0.2442  avg train accu: 0.932\n",
      "epoch: 194  avg test  loss: 0.1409  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 195  batch:    0  loss: 0.3090\n",
      "epoch: 195  batch:  100  loss: 0.2935\n",
      "epoch: 195  batch:  200  loss: 0.2791\n",
      "epoch: 195  batch:  300  loss: 0.2190\n",
      "epoch: 195  batch:  400  loss: 0.3068\n",
      "epoch: 195  batch:  500  loss: 0.2946\n",
      "epoch: 195  batch:  600  loss: 0.2099\n",
      "epoch: 195  batch:  700  loss: 0.1640\n",
      "epoch: 195  batch:  800  loss: 0.2775\n",
      "epoch: 195  batch:  900  loss: 0.2086\n",
      "epoch: 195  avg train loss: 0.2426  avg train accu: 0.931\n",
      "epoch: 195  avg test  loss: 0.1411  avg test  accu: 0.958\n",
      "==================================================================================\n",
      "epoch: 196  batch:    0  loss: 0.1851\n",
      "epoch: 196  batch:  100  loss: 0.1597\n",
      "epoch: 196  batch:  200  loss: 0.2391\n",
      "epoch: 196  batch:  300  loss: 0.2323\n",
      "epoch: 196  batch:  400  loss: 0.1792\n",
      "epoch: 196  batch:  500  loss: 0.2824\n",
      "epoch: 196  batch:  600  loss: 0.3147\n",
      "epoch: 196  batch:  700  loss: 0.3150\n",
      "epoch: 196  batch:  800  loss: 0.2287\n",
      "epoch: 196  batch:  900  loss: 0.2432\n",
      "epoch: 196  avg train loss: 0.2448  avg train accu: 0.932\n",
      "epoch: 196  avg test  loss: 0.1416  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 197  batch:    0  loss: 0.3302\n",
      "epoch: 197  batch:  100  loss: 0.1410\n",
      "epoch: 197  batch:  200  loss: 0.2940\n",
      "epoch: 197  batch:  300  loss: 0.2415\n",
      "epoch: 197  batch:  400  loss: 0.3403\n",
      "epoch: 197  batch:  500  loss: 0.2393\n",
      "epoch: 197  batch:  600  loss: 0.3132\n",
      "epoch: 197  batch:  700  loss: 0.2361\n",
      "epoch: 197  batch:  800  loss: 0.2722\n",
      "epoch: 197  batch:  900  loss: 0.1817\n",
      "epoch: 197  avg train loss: 0.2448  avg train accu: 0.931\n",
      "epoch: 197  avg test  loss: 0.1403  avg test  accu: 0.960\n",
      "==================================================================================\n",
      "epoch: 198  batch:    0  loss: 0.1337\n",
      "epoch: 198  batch:  100  loss: 0.2217\n",
      "epoch: 198  batch:  200  loss: 0.2545\n",
      "epoch: 198  batch:  300  loss: 0.3041\n",
      "epoch: 198  batch:  400  loss: 0.1746\n",
      "epoch: 198  batch:  500  loss: 0.2778\n",
      "epoch: 198  batch:  600  loss: 0.1722\n",
      "epoch: 198  batch:  700  loss: 0.3521\n",
      "epoch: 198  batch:  800  loss: 0.2576\n",
      "epoch: 198  batch:  900  loss: 0.2272\n",
      "epoch: 198  avg train loss: 0.2431  avg train accu: 0.931\n",
      "epoch: 198  avg test  loss: 0.1423  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 199  batch:    0  loss: 0.2012\n",
      "epoch: 199  batch:  100  loss: 0.2273\n",
      "epoch: 199  batch:  200  loss: 0.2172\n",
      "epoch: 199  batch:  300  loss: 0.2437\n",
      "epoch: 199  batch:  400  loss: 0.2008\n",
      "epoch: 199  batch:  500  loss: 0.2173\n",
      "epoch: 199  batch:  600  loss: 0.2441\n",
      "epoch: 199  batch:  700  loss: 0.3875\n",
      "epoch: 199  batch:  800  loss: 0.1564\n",
      "epoch: 199  batch:  900  loss: 0.2922\n",
      "epoch: 199  avg train loss: 0.2446  avg train accu: 0.930\n",
      "epoch: 199  avg test  loss: 0.1407  avg test  accu: 0.959\n",
      "==================================================================================\n",
      "epoch: 200  batch:    0  loss: 0.1727\n",
      "epoch: 200  batch:  100  loss: 0.1603\n",
      "epoch: 200  batch:  200  loss: 0.1585\n",
      "epoch: 200  batch:  300  loss: 0.4119\n",
      "epoch: 200  batch:  400  loss: 0.2449\n",
      "epoch: 200  batch:  500  loss: 0.3132\n",
      "epoch: 200  batch:  600  loss: 0.3157\n",
      "epoch: 200  batch:  700  loss: 0.2605\n",
      "epoch: 200  batch:  800  loss: 0.1644\n",
      "epoch: 200  batch:  900  loss: 0.1280\n",
      "epoch: 200  avg train loss: 0.2436  avg train accu: 0.931\n",
      "epoch: 200  avg test  loss: 0.1417  avg test  accu: 0.960\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "num_epoch = 200\n",
    "print_batch_period = 100\n",
    "learning_rate = 1e-4\n",
    "l2_regularization = 0.1\n",
    "\n",
    "# initialization of model, optimizer and loss function\n",
    "model = Vanilla_MLP().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = l2_regularization)\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "train_loss = []\n",
    "train_accu = []\n",
    "test_loss  = []\n",
    "test_accu  = []\n",
    "\n",
    "# training loop\n",
    "for epoch in notebook.tqdm(range(num_epoch)):\n",
    "    # train\n",
    "    model.train()\n",
    "    sum_batch_loss = 0\n",
    "    num_correct_preds = 0\n",
    "    for batch_id, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        predicts = model(images)\n",
    "        \n",
    "        loss = loss_func(predicts, labels, reduction = 'sum')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # training loss and accuracy of one mini-batch\n",
    "        sum_batch_loss += loss.item()\n",
    "        _, pred_labels = torch.max(predicts, 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        num_correct_preds += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        \n",
    "        # print per batch_period\n",
    "        if batch_id % print_batch_period == 0:\n",
    "            print(\"epoch: {:02d}\".format(epoch+1), \" batch: {:4d}\".format(batch_id), \" loss: {:.4f}\".format(loss.item()/len(labels)))\n",
    "            \n",
    "    avg_loss = sum_batch_loss / len(train_dataset)\n",
    "    train_loss.append(avg_loss)\n",
    "    \n",
    "    avg_accu = num_correct_preds / len(train_dataset)\n",
    "    train_accu.append(avg_accu)\n",
    "    \n",
    "    # print per epoch\n",
    "    print(\"epoch: {:02d}\".format(epoch+1), \" avg train loss: {:.4f}\".format(avg_loss), \" avg train accu: {:.3f}\".format(avg_accu))\n",
    "    \n",
    "    # test\n",
    "    # with torch.no_grad():\n",
    "    model.eval()\n",
    "    sum_batch_loss = 0\n",
    "    num_correct_preds = 0\n",
    "    for batch_id, (images, labels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        predicts = model(images)\n",
    "\n",
    "        # test loss of one mini-batch\n",
    "        loss = loss_func(predicts, labels, reduction = 'sum')\n",
    "        sum_batch_loss += loss.item()\n",
    "        \n",
    "        # test accuracy of one mini-batch\n",
    "        _, pred_labels = torch.max(predicts, 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        num_correct_preds += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        \n",
    "    avg_loss = sum_batch_loss / len(test_dataset)\n",
    "    test_loss.append(avg_loss)\n",
    "    \n",
    "    avg_accu = num_correct_preds / len(test_dataset)\n",
    "    test_accu.append(avg_accu)\n",
    "    \n",
    "    # print per epoch\n",
    "    print(\"epoch: {:02d}\".format(epoch+1), \" avg test  loss: {:.4f}\".format(avg_loss), \" avg test  accu: {:.3f}\".format(avg_accu))\n",
    "    print(\"==================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa519e3c-eabe-4f69-ae81-9948b94a82ee",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1bf1dfa8-896d-4e79-9915-6560ca6bd472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (16, 4)) \n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epoch + 1), train_loss, label = 'train')\n",
    "plt.plot(range(1, num_epoch + 1), test_loss , label = 'test')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epoch + 1), train_accu, label = 'train')\n",
    "plt.plot(range(1, num_epoch + 1), test_accu , label = 'test')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
